{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee699fac-d9d5-4a3d-8684-3887c3afdd9c",
   "metadata": {},
   "source": [
    "# Make dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60886d4c-a1f0-409f-a3b6-1dcfd207bdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aa8417/.conda/envs/dna/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    AutoModelForSequenceClassification, \n",
    "    DataCollatorForTokenClassification, \n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from transformers.models.bert.configuration_bert import BertConfig \n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from training_args_module import training_args\n",
    "import transformers\n",
    "import os\n",
    "import csv\n",
    "from Bio import SeqIO\n",
    "import argparse\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a7c1e6-7515-4f80-9f02-60bbcdb9ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "princeton_id = 'aa8417'\n",
    "project_dir = f'/scratch/gpfs/{princeton_id}/QCB557_project'\n",
    "\n",
    "# use gpu\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_out_dir = '/scratch/gpfs/aa8417/QCB557_project/models/replicate_043024/rep1/fine_tune_parallel_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41da3abd-4419-4a01-8ffe-af0ba13b317e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_seq = BertConfig.from_pretrained(f'/scratch/gpfs/{princeton_id}/QCB557_project/models/replicate_043024/rep1/fine_tune_parallel_v1/config.json', output_attentions=True)\n",
    "print(config_seq.num_labels) #2 labels\n",
    "model_seq = AutoModelForSequenceClassification.from_pretrained(model_out_dir, trust_remote_code=True, config=config_seq)\n",
    "model_seq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9077f47b-77c4-49eb-9305-c22f671e553b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertUnpadAttention(\n",
       "          (self): BertUnpadSelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): BertGatedLinearUnitMLP(\n",
       "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertUnpadAttention(\n",
       "          (self): BertUnpadSelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): BertGatedLinearUnitMLP(\n",
       "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertUnpadAttention(\n",
       "          (self): BertUnpadSelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): BertGatedLinearUnitMLP(\n",
       "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertUnpadAttention(\n",
       "          (self): BertUnpadSelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): BertGatedLinearUnitMLP(\n",
       "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertUnpadAttention(\n",
       "          (self): BertUnpadSelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): BertGatedLinearUnitMLP(\n",
       "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertUnpadAttention(\n",
       "          (self): BertUnpadSelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): BertGatedLinearUnitMLP(\n",
       "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertUnpadAttention(\n",
       "          (self): BertUnpadSelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): BertGatedLinearUnitMLP(\n",
       "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertUnpadAttention(\n",
       "          (self): BertUnpadSelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): BertGatedLinearUnitMLP(\n",
       "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertUnpadAttention(\n",
       "          (self): BertUnpadSelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): BertGatedLinearUnitMLP(\n",
       "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertUnpadAttention(\n",
       "          (self): BertUnpadSelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): BertGatedLinearUnitMLP(\n",
       "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertUnpadAttention(\n",
       "          (self): BertUnpadSelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): BertGatedLinearUnitMLP(\n",
       "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertUnpadAttention(\n",
       "          (self): BertUnpadSelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): BertGatedLinearUnitMLP(\n",
       "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_base = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\", output_hidden_states=True, output_attentions=True)\n",
    "print(config_base.num_labels) #2 labels\n",
    "model_base = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, config=config_base)\n",
    "model_base.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfeac853-8d13-4513-bc58-d28c1d9470d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, padding=True)\n",
    "tokenizer.pad_token = \"X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d0a20ab-6f36-48f3-af5b-9f52384004c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(f'/scratch/gpfs/{princeton_id}/QCB557_project/data/H3K4me3/test.csv')\n",
    "train = pd.read_csv(f'/scratch/gpfs/{princeton_id}/QCB557_project/data/H3K4me3/train.csv')\n",
    "full = pd.concat([train, test], ignore_index=True)\n",
    "full.to_csv(f'/scratch/gpfs/{princeton_id}/QCB557_project/data/H3K4me3/all_seqs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4f53843-fde4-4c51-9f4f-117619c070f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_output(model_base, model_seq, tokenizer, dataframe, device):\n",
    "    results_dict = {}\n",
    "    counter = 0\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        sequence = row['sequence']\n",
    "        label = row['label']\n",
    "        \n",
    "        inputs = tokenizer(sequence, padding='max_length', max_length=128, return_tensors='pt').to(device)\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_seq = model_seq(input_ids=input_ids, attention_mask=attention_mask, output_attentions=True)\n",
    "            hidden_states = outputs_seq.hidden_states\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs_base = model_base(input_ids=input_ids, attention_mask=attention_mask, output_attentions=True)\n",
    "            attention_weights = outputs_base[1]\n",
    "\n",
    "        key = f'seq_{index}_{label}'\n",
    "        \n",
    "        results_dict[key] = {\n",
    "            \"sequence\": sequence,\n",
    "            \"input_ids\": input_ids.cpu().detach().numpy().tolist(),\n",
    "            \"hidden_states\": [tensor.cpu().detach().numpy() for tensor in hidden_states],\n",
    "            \"attention_weights\": attention_weights.cpu().detach().numpy().tolist()\n",
    "        }\n",
    "        \n",
    "        counter += 1\n",
    "        percent = (counter/36799)*100\n",
    "        \n",
    "        if counter % 1000 == 0:\n",
    "            print(f\"{percent}% complete!\")\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ae3c74-0c2c-4c3b-ae31-a19f65eda6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7174651485094703% complete!\n",
      "5.434930297018941% complete!\n",
      "8.152395445528411% complete!\n",
      "10.869860594037881% complete!\n",
      "13.587325742547351% complete!\n",
      "16.304790891056822% complete!\n",
      "19.022256039566294% complete!\n",
      "21.739721188075762% complete!\n",
      "24.45718633658523% complete!\n",
      "27.174651485094703% complete!\n",
      "29.892116633604175% complete!\n",
      "32.609581782113644% complete!\n",
      "35.32704693062311% complete!\n",
      "38.04451207913259% complete!\n",
      "40.761977227642056% complete!\n",
      "43.479442376151525% complete!\n",
      "46.19690752466099% complete!\n",
      "48.91437267317046% complete!\n",
      "51.63183782167994% complete!\n",
      "54.349302970189406% complete!\n",
      "57.066768118698874% complete!\n",
      "59.78423326720835% complete!\n",
      "62.50169841571782% complete!\n",
      "65.21916356422729% complete!\n",
      "67.93662871273676% complete!\n",
      "70.65409386124622% complete!\n",
      "73.3715590097557% complete!\n",
      "76.08902415826518% complete!\n",
      "78.80648930677464% complete!\n",
      "81.52395445528411% complete!\n",
      "84.24141960379357% complete!\n",
      "86.95888475230305% complete!\n",
      "89.67634990081253% complete!\n",
      "92.39381504932199% complete!\n",
      "95.11128019783146% complete!\n",
      "97.82874534634092% complete!\n"
     ]
    }
   ],
   "source": [
    "results_dict = get_model_output(model_base, model_seq, tokenizer, full, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "678229f0-c90c-431f-be6b-532934e3e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_pickle(results_dict, file_path):\n",
    "    with open(file_path, 'wb') as pickle_file:\n",
    "        pickle.dump(results_dict, pickle_file)\n",
    "\n",
    "file_path = f'/scratch/gpfs/{princeton_id}/QCB557_project/data/H3K4me3/results_dict.pkl'\n",
    "save_to_pickle(results_dict, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "760f8dfb-b834-40d4-bd99-40df23272adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key content:\n",
      "{'sequence': 'ACTACAACTGTTTAGCTACTAACTTGCTTGACATCCAAAGAGCAGGCCAATCACTAGAGAATGTCTGGGATCACTCAGATGGGTACACACAAGGCAGTGGATGTGGTTGTTACCGCCACGACTAGGGTCACTCTCTACCGTGAATTTCGAAATTTCGCGAACTGAGTGCGCCAGCTAGCCATTGCTACCAAGCTAACAAAAGGATCAGGCTGCCCAAACGGACGTAGACTCACTGGGCTCCGTCTAGAGAATGACACGAAACCTCACTCGTTGTTTCCTTCCGAGTGATTAACTAGACGGGAATACTGCCTGGAAATTTTTTCGGAAGAGAAAGTTTCGAATTTTAGTTATTCAGTTTCCCGAAAAAGGAATAAAATATGTACAGATGTAGTTGATCAAAAAAAAGTCTATGGACATCATTCCTACTGAAGGCACAGCTAGTACCTTATTTATTGCTGATGCTTTAGCAGATTTAATCAACTGTCAACAGCAGTGATATT', 'input_ids': [[1, 5, 770, 2084, 105, 79, 1645, 100, 388, 208, 1113, 759, 2059, 54, 1152, 63, 83, 138, 2242, 112, 483, 42, 2222, 3305, 93, 681, 67, 2032, 13, 1276, 16, 286, 685, 2350, 30, 504, 105, 475, 105, 76, 233, 96, 1152, 189, 652, 174, 194, 933, 277, 78, 41, 2039, 414, 409, 190, 40, 395, 130, 13, 875, 84, 37, 846, 25, 236, 74, 45, 122, 446, 299, 31, 480, 69, 77, 115, 736, 2121, 57, 58, 981, 55, 106, 512, 195, 39, 2302, 100, 127, 114, 119, 912, 105, 1591, 110, 43, 1749, 29, 275, 73, 1537, 2757, 148, 61, 43, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'hidden_states': [array([[-0.13585961, -0.16009824,  0.24658795, ...,  0.19274357,\n",
      "         0.45163104, -0.02409274],\n",
      "       [ 0.04392688,  0.09860186,  0.07619097, ...,  0.5046952 ,\n",
      "         0.3635674 ,  0.0090975 ],\n",
      "       [-0.12809874, -0.15654317, -0.13669735, ...,  0.22383708,\n",
      "         0.02991295,  0.25569236],\n",
      "       ...,\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ]], dtype=float32)], 'attention_weights': [[0.1505299061536789, -0.19011420011520386, -0.1874278038740158, 0.01715671271085739, 0.060301948338747025, -0.04634295031428337, -0.019723298028111458, -0.03438873961567879, 0.2042103260755539, -0.0526873841881752, -0.04356013238430023, 0.12910453975200653, -0.131765216588974, -0.031645968556404114, 0.05569637194275856, 0.12678395211696625, -0.005296374671161175, -0.08494387567043304, -0.08644890040159225, -0.02706627920269966, -0.12342964857816696, 0.09540030360221863, -0.18755978345870972, -0.1032019630074501, 0.23777338862419128, -0.22210894525051117, 0.0665922537446022, -0.0027283995877951384, 0.23031827807426453, -0.1535607874393463, -0.28390970826148987, -0.16042473912239075, 0.17728900909423828, 0.09974754601716995, -0.11613396555185318, -0.008172268979251385, -0.03369063138961792, -0.16064365208148956, 0.03567083179950714, 0.19776082038879395, -0.0476105734705925, 0.02547825686633587, 0.14295266568660736, -0.02141718752682209, -0.055472616106271744, -0.0741008073091507, -0.07040095329284668, -0.030050860717892647, -0.034135300666093826, 0.12248769402503967, 0.06396394222974777, -0.012113941833376884, -0.16555830836296082, 0.17493967711925507, -0.07217259705066681, 0.22579388320446014, -0.18786127865314484, -0.028621498495340347, 0.0802452564239502, -0.13899342715740204, -0.04468323290348053, 0.01656216010451317, 0.11594770848751068, -0.01497492752969265, -0.10156740248203278, 0.08610371500253677, 0.1089508906006813, -0.1525360345840454, 0.09932933002710342, 0.2557719647884369, -0.23073574900627136, 0.18447557091712952, 0.03953412175178528, -0.026289844885468483, 0.10950710624456406, 0.02010904997587204, 0.13082103431224823, -0.08054148405790329, -0.058322299271821976, 0.10839752852916718, -0.08690214902162552, 0.08500119298696518, 0.10098957270383835, 0.13238488137722015, -0.19942538440227509, -0.14552238583564758, 0.19821953773498535, -0.12871181964874268, 0.22068482637405396, 0.11532090604305267, -0.14661389589309692, -0.03415622562170029, 0.07614685595035553, -0.09341451525688171, -0.17380908131599426, 0.00795738771557808, 0.16206642985343933, -0.0868840292096138, 0.13531643152236938, -0.053591638803482056, -0.1699749082326889, 0.06222444772720337, -0.01223742961883545, -0.042992014437913895, -0.06481938064098358, 0.0688888356089592, -0.07277534157037735, 0.03886804357171059, 0.09682773798704147, 0.03445976972579956, 0.15233056247234344, 0.06072871387004852, -0.07866208255290985, -0.1935478299856186, 0.2004712074995041, -0.19917011260986328, -0.035892192274332047, 0.21264994144439697, 0.06366411596536636, -0.17957960069179535, -0.10248193144798279, -0.07327886670827866, -0.03165596351027489, 0.12525314092636108, -0.04186118766665459, 0.1912137269973755, -0.11023929715156555, 0.032863304018974304, 0.022561470046639442, -0.014370767399668694, -0.040100108832120895, -0.05143830180168152, 0.047921884804964066, -0.09714975953102112, 0.04310540482401848, -0.013347766362130642, -0.1458989977836609, 0.03352446109056473, 0.05821710079908371, -0.035353317856788635, 0.12384218722581863, -0.06961233168840408, 0.10033728182315826, 0.22237372398376465, 0.055670175701379776, -0.14111746847629547, 0.000833835278172046, -0.2191983461380005, 0.07618042081594467, -0.20132018625736237, 0.014220408163964748, 0.1342880129814148, -0.015369728207588196, 0.027330974116921425, -0.04520976543426514, -0.10969534516334534, 0.06082345172762871, 0.14891590178012848, 0.09749580174684525, 0.24957838654518127, 0.20046746730804443, -0.07422242313623428, 0.07046358287334442, -0.10791188478469849, -0.04597737267613411, 0.02662729099392891, -0.05013861879706383, 0.0029190657660365105, -0.20106768608093262, 0.05853293836116791, 0.3048604130744934, 0.049528900533914566, -0.04272002354264259, 0.10810808837413788, -0.02526874653995037, -0.0014329710975289345, 0.09774570167064667, 0.1210249736905098, 0.12862364947795868, -0.03711750730872154, 0.05159618705511093, 0.10286914557218552, -0.0019543415401130915, 0.08666961640119553, -0.0002817884087562561, -0.016553591936826706, 0.33994078636169434, 0.17085012793540955, -0.040893688797950745, -0.006280208472162485, 0.0702773928642273, 0.005488310940563679, 0.213258758187294, 0.13926592469215393, -0.06330915540456772, -0.21539567410945892, -0.10110563039779663, 0.18771377205848694, -0.027990613132715225, 0.039458490908145905, 0.08162893354892731, -0.2078658938407898, 0.018619658425450325, -0.06049100682139397, -0.04412157088518143, 0.026474663987755775, -0.023580653592944145, -0.09381421655416489, -0.04412182420492172, 0.04070207476615906, -0.022188063710927963, 0.11162639409303665, -0.04563160985708237, 0.11846920847892761, 0.02446114458143711, -0.21046258509159088, -0.009757832624018192, 0.05793684720993042, 0.06924334168434143, -0.10491906106472015, -0.06559926271438599, -0.08712838590145111, 0.19887009263038635, -0.08961094170808792, -0.0031337542459368706, 0.015205508098006248, 0.2081405222415924, 0.14202575385570526, 0.0058695352636277676, -0.08869252353906631, 0.0027726965490728617, -0.027397077530622482, -0.25088006258010864, -0.07288381457328796, -0.03078356571495533, 0.11018779128789902, -0.33373090624809265, 0.09004167467355728, -0.11704418808221817, -0.13004948198795319, -0.048285435885190964, 0.0023150178603827953, 0.163309708237648, 0.09339255094528198, -0.31447312235832214, -0.17494480311870575, 0.21051639318466187, 0.01675405353307724, 0.01902008056640625, -0.02593163400888443, -0.1221521869301796, 0.043419186025857925, -0.06273304671049118, 0.08326912671327591, 0.023560380563139915, 0.18852445483207703, -0.21079152822494507, 0.0640626922249794, -0.034805163741111755, 0.046246305108070374, -0.15114593505859375, -0.23691119253635406, 0.08500391244888306, 0.1673150211572647, -0.073213592171669, 0.00795942172408104, -0.12067881226539612, 0.14225329458713531, 0.047965094447135925, 0.07613057643175125, -0.017761019989848137, 0.06518919765949249, 0.14420606195926666, 0.15798720717430115, 0.007052770350128412, -0.04894091188907623, -0.11150934547185898, -0.04038703441619873, 0.09845283627510071, -0.10402187705039978, 0.17184676229953766, -0.3293153941631317, 0.14706671237945557, 0.06018665060400963, 0.10655736178159714, -0.180165633559227, 0.06007780134677887, -0.07668573409318924, -0.20311179757118225, 0.16113106906414032, -0.06923512369394302, 0.045353762805461884, -0.33689144253730774, -0.109281025826931, -0.07405330240726471, 0.12883958220481873, 0.041647523641586304, 0.12452805787324905, 0.20057915151119232, -0.056601788848638535, 0.04645155742764473, 0.03316330164670944, -0.03077603317797184, -0.11299130320549011, 0.1662631630897522, -0.015376972034573555, 0.09994102269411087, -0.06458615511655807, -0.20515091717243195, -0.028412029147148132, -0.11925394833087921, 0.010095955803990364, -0.01180341374129057, 0.09575002640485764, -0.030788201838731766, -0.00833948701620102, 0.024034054949879646, 0.06357281655073166, -0.04142304137349129, -0.004099720623344183, -0.0751839131116867, 0.04045640677213669, 0.08181015402078629, -0.19046124815940857, -0.12897135317325592, 0.04110855236649513, 0.08690767735242844, 0.0018951945239678025, -0.04034169018268585, 0.1801600605249405, 0.1472225934267044, 0.11152821779251099, 0.08310381323099136, -0.03549502044916153, -0.08467350155115128, 0.04507151618599892, -0.07325061410665512, -0.12636850774288177, -0.019714951515197754, -0.010701456107199192, 0.01760452799499035, 0.037497323006391525, 0.10063263773918152, 0.045351386070251465, -0.34286996722221375, 0.059708744287490845, 0.16135185956954956, -0.034933652728796005, -0.21102838218212128, -0.07411129772663116, -0.04101407900452614, -0.03680345043540001, -0.138803169131279, -0.16117434203624725, -0.04865305498242378, 0.05434054136276245, 0.033145464956760406, -0.1959667056798935, -0.16872729361057281, 0.08596422523260117, 0.03553878143429756, -0.19394560158252716, -0.3230608105659485, -0.0732164978981018, 0.05312584713101387, 0.2914844751358032, 0.05804571136832237, 0.112818144261837, 0.0523122102022171, -0.14567764103412628, -0.007795168552547693, 0.11134012788534164, -0.07436197251081467, 0.02536633238196373, 0.01680677756667137, -0.13012078404426575, 0.16917899250984192, -0.049892060458660126, 0.09691635519266129, 0.1349615603685379, 0.1502406746149063, 0.023593710735440254, -0.1271398812532425, -0.10722512751817703, 0.1399669647216797, -0.20232442021369934, -0.17314721643924713, 0.019784346222877502, 0.07989825308322906, 0.02195148728787899, -0.038936492055654526, -0.1517331451177597, 0.03622807562351227, 0.013720106333494186, 0.16569022834300995, -0.19411087036132812, -0.017061380669474602, -0.1749662458896637, -0.12916582822799683, -0.2016926258802414, 0.06217048689723015, -0.08247475326061249, -0.09218788146972656, -0.10554926097393036, 0.09762270748615265, -0.17044368386268616, -0.07839132845401764, -0.18669942021369934, 0.1426931619644165, 0.06813984364271164, -0.09251178056001663, -0.12455502152442932, -0.022974153980612755, 0.13564704358577728, 0.06299594044685364, -0.14463849365711212, 0.06188589707016945, -0.040499426424503326, 0.022405944764614105, 0.07331223040819168, -0.014888942241668701, -0.11019045114517212, -0.02231191098690033, 0.0521782711148262, 0.17015430331230164, -0.03124888986349106, -0.00372782489284873, 0.18610110878944397, 0.001081101130694151, -0.09666385501623154, 0.18400651216506958, 0.00013629533350467682, 0.0927833840250969, -0.07122456282377243, 0.15316742658615112, 0.04037479683756828, 0.2469862997531891, 0.24722684919834137, -0.11113718152046204, 0.26617300510406494, -0.0849444791674614, -0.10916271805763245, -0.1688707023859024, 0.02246583066880703, -0.029826557263731956, 0.10692846775054932, 0.06825236976146698, 0.37823423743247986, 0.11377165466547012, -0.003955351188778877, -0.2406972050666809, -0.19179242849349976, 0.08957833051681519, -0.07886098325252533, 0.232853502035141, 0.05192309990525246, 0.1520555317401886, -0.15067201852798462, 0.10896933823823929, 0.21000508964061737, 0.08720394968986511, -0.05815482512116432, 0.1497897505760193, 0.03620212897658348, -0.11729097366333008, -0.03149685263633728, 0.11511315405368805, -0.10860595852136612, 0.09681234508752823, 0.005017879419028759, -0.007905115373432636, -0.062411870807409286, -0.17060770094394684, -0.07506260275840759, 0.16196055710315704, 0.044804897159338, 0.0785035565495491, 0.1430870145559311, -0.051564961671829224, 0.09357403963804245, -0.017223242670297623, 0.02194754034280777, 0.028907982632517815, -0.07031091302633286, -0.22991609573364258, -0.008650502189993858, -0.15897713601589203, 0.06773319095373154, -0.019830096513032913, 0.019915588200092316, 0.04558415710926056, 0.00921446830034256, 0.07411344349384308, 0.058650970458984375, -0.14955492317676544, -0.058842942118644714, 0.10268459469079971, 0.1376798301935196, -0.0011752689024433494, 0.05969633162021637, -0.002416369505226612, 0.056692514568567276, 0.06474970281124115, 0.11933563649654388, -0.08062122017145157, 0.07660295069217682, -0.09448936581611633, -0.04055022448301315, -0.10780362039804459, 0.07610273361206055, 0.16014358401298523, -0.18133847415447235, -0.07642349600791931, 0.25970855355262756, -0.05434207618236542, -0.28864797949790955, 0.0038098508957773447, -0.15833015739917755, -0.06783586740493774, 0.10450881719589233, -0.026775268837809563, -0.19594047963619232, -0.09749836474657059, 0.1954328715801239, 0.0059265694580972195, 0.09477172046899796, 0.005014018155634403, -0.15828600525856018, -0.24128341674804688, 0.033895354717969894, 0.1304984986782074, 0.07961991429328918, 0.02015787735581398, -0.03527205064892769, 0.15399058163166046, -0.0021086137276142836, 0.020188506692647934, 0.11164486408233643, 0.008416864089667797, -0.10651114583015442, -0.06709452718496323, 0.12316060066223145, -0.0839286744594574, 0.06633413583040237, -0.04521922767162323, -0.019113678485155106, -0.25027188658714294, -0.11312665790319443, 0.11475852131843567, 0.1875065118074417, 0.05437599495053291, 0.0877545028924942, -0.020524919033050537, -0.11943195015192032, -0.11617430299520493, 0.07953590899705887, 0.20322895050048828, -0.016912203282117844, -0.008343611843883991, 0.057782646268606186, 0.005878164898604155, -0.0855511948466301, -0.17800362408161163, 0.08021912723779678, -0.09353464096784592, 0.11557639390230179, 0.0765429139137268, 0.029053019359707832, -0.21427546441555023, 0.03176087141036987, 0.06088760122656822, -0.002481496427208185, -0.1742139309644699, -0.08052897453308105, -0.0647389218211174, -0.13274702429771423, 0.2685619592666626, 0.007935118861496449, 0.10664013773202896, 0.2595841586589813, 0.03595810756087303, 0.07748928666114807, -0.1239098533987999, -0.02888563461601734, -0.16437487304210663, 0.07486625015735626, -0.13988178968429565, 0.09549574553966522, 0.09039395302534103, 0.21194733679294586, -0.178566575050354, 0.01360573060810566, 0.18015633523464203, -0.08910785615444183, -0.1010015532374382, 0.009687687270343304, -0.013147907331585884, 0.06447937339544296, -0.025243788957595825, 0.03602340817451477, -0.028974130749702454, -0.23510277271270752, 0.11682333052158356, -0.020051242783665657, 0.01741691678762436, 0.1920078694820404, -0.11064580827951431, 0.06181631609797478, -0.11504101008176804, -0.16431298851966858, 0.03724218159914017, 0.0013538479106500745, 0.0595364011824131, 0.09046807885169983, 0.0772102028131485, 0.22264790534973145, -0.15609022974967957, 0.13813936710357666, 0.016763893887400627, -0.18459118902683258, -0.1381894201040268, 0.10405457764863968, -0.051451101899147034, -0.09578786045312881, 0.03554622456431389, 0.020576931536197662, 0.1564752161502838, -0.0783492848277092, -0.040046773850917816, 0.13673853874206543, -0.06097062677145004, 0.06515063345432281, -0.12399479746818542, -0.005411728750914335, -0.10306885838508606, -0.14100299775600433, 0.11207310855388641, -0.04054466634988785, 0.09553797543048859, -0.11653602868318558, -0.150517538189888, 0.2035006284713745, -0.21176564693450928, -0.07738322019577026, 0.036543410271406174, 0.04518267512321472, 0.08005698025226593, -0.1461041420698166, -0.20668983459472656, 0.1852412223815918, -0.14942185580730438, -0.06698103249073029, 0.1660502552986145, 0.05124560743570328, 0.017356369644403458, -0.2118150293827057, 0.22233876585960388, 0.09673376381397247, 0.03198133036494255, -0.1295558661222458, 0.0855472981929779, 0.11817510426044464, -0.150945782661438, -0.44975098967552185, -0.018742237240076065, -0.14482218027114868, 0.1352708339691162, 0.09523438662290573, -0.13624858856201172, -0.1242033913731575, -0.13606496155261993, 0.08076177537441254, -0.009371493943035603, 0.08627620339393616, 0.0551719032227993, 0.0037646377459168434, 0.1428578495979309, 0.1146145835518837, -0.07406273484230042, 0.21422839164733887, -0.05022566020488739, -0.023304592818021774, -0.18274442851543427, 0.0353718101978302, 0.036844927817583084, -0.03640099614858627, 0.05362868681550026, 0.02141546458005905, -0.021122856065630913, 0.0850512832403183, 0.003721048589795828, -0.18031521141529083, -0.008802512660622597, 0.010001184418797493, 0.08232182264328003, -0.07642596960067749, 0.11419828236103058, -0.09379632771015167, 0.08922697603702545, 0.018406569957733154, 0.09669142961502075, -0.06165284290909767, 0.1628485471010208, 0.06382215023040771, -0.004940974526107311, -0.17248138785362244, 0.05966579541563988, 0.1930614709854126, 0.0188532005995512, 0.12260008603334427, -0.06324408203363419, -0.18337944149971008, 0.04454760625958443, 0.0461365208029747, 0.026656584814190865, 0.2288396805524826, -0.22874236106872559, -0.10775447636842728, -0.12638692557811737, -0.13956664502620697, 0.18155258893966675, 0.12682732939720154, 0.02335645817220211, 0.1426592618227005, -0.16459789872169495, 0.0516284704208374, -0.14575240015983582, 0.04785206913948059, 8.763745427131653e-05, 0.15046201646327972, -0.007410778198391199, 0.06771436333656311, -0.04997926205396652, 0.10831716656684875, -0.09815853834152222, -0.16761058568954468, -0.01250743493437767, -0.05761870741844177, -0.14324639737606049, -0.12853732705116272, -0.0890057310461998, 0.1645670384168625, -0.20928546786308289, -0.09781172126531601, -0.09599768370389938, -0.09426333010196686, 0.0485299751162529, 0.07022032886743546, 0.08365841954946518, 0.08197461068630219, 0.039152320474386215, -0.09038369357585907, 0.11370884627103806, 0.10228051990270615, -0.027315272018313408, 0.03833705931901932, -0.028015753254294395, -0.02973421849310398, 0.05167503282427788, -0.17612935602664948, -0.03650078549981117, 0.04253759607672691, -0.09983960539102554, -0.15447241067886353, -0.04172591492533684, -0.08151988685131073, 0.12284163385629654, 0.04809323325753212]]}\n",
      "\n",
      "Last key content:\n",
      "{'sequence': 'TGTCAAGCAATTATTATTACTGTGGCATAAGCAGGGCTATAAAGCCCTACTCTTCACTCAATCGAGACAAATGCTCGATATTCTAGAGGAGTTTATATCAACGAAAGATCCCGATTTATCACATTTGAACTACCTACGAATGGATGGGACAACTAATATCAAAGGGAGACAGTCGTTAGTTGACCGGTTTAACAATGAATCTTTCGATGTATTCTTACTAACCACAAGAGTTGGGGGGCTAGGTGTTAATTTAACCGGTGCTAATAGAATTATCATTTTTGATCCAGACTGGAACCCATCTACTGACATGCAAGCTCGAGAAAGGGCATGGAGGATTGGGCAGAAAAGAGAGGTATCAATATATAGGTTGATGGTAGGAGGTTCAATAGAGGAGAAGATTTATCATAGACAAATATTCAAACAATTCTTAACCAACAGGATTTTGACTGATCCTAAGCAGAAACGATTTTTCAAAATTCACGAACTCCATGATCTGTTTT', 'input_ids': [[1, 11, 56, 66, 273, 24, 180, 1376, 253, 105, 38, 1268, 1619, 164, 1154, 373, 40, 657, 37, 191, 1712, 56, 409, 375, 429, 211, 3566, 37, 114, 2174, 83, 1016, 79, 2433, 747, 369, 77, 31, 158, 101, 449, 52, 47, 1484, 3980, 79, 709, 222, 92, 422, 15, 2368, 615, 103, 79, 207, 24, 625, 23, 462, 632, 95, 88, 59, 3976, 239, 299, 332, 108, 216, 332, 85, 50, 138, 56, 46, 730, 83, 1416, 101, 3182, 383, 73, 211, 98, 2647, 49, 162, 150, 76, 203, 21, 910, 26, 695, 45, 3368, 195, 115, 166, 40, 246, 1679, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'hidden_states': [array([[-0.10837534,  0.04809872,  0.11437479, ..., -0.20162196,\n",
      "         0.3249517 ,  0.03409604],\n",
      "       [ 0.21451019,  0.1073227 ,  0.2904556 , ...,  0.16729505,\n",
      "         0.2572187 , -0.24275076],\n",
      "       [-0.09584805,  0.0077837 , -0.00093995, ..., -0.01713452,\n",
      "         0.2819331 ,  0.37582377],\n",
      "       ...,\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ]], dtype=float32)], 'attention_weights': [[0.0033991914242506027, -0.13759173452854156, -0.019541343674063683, 0.1077185645699501, -0.0096767358481884, 0.0831197202205658, 0.034129753708839417, 0.050727132707834244, 0.1913682371377945, 0.07578328251838684, 0.0542549230158329, 0.17989303171634674, -0.11283641308546066, -0.05762472376227379, -0.042644210159778595, -0.010205410420894623, -0.15709777176380157, -0.021372631192207336, 0.06588713079690933, -0.002249065786600113, -0.02293228544294834, 0.08736510574817657, 0.038877326995134354, 0.08458790183067322, 0.1470041275024414, -0.14306996762752533, 0.02689329907298088, 0.09415270388126373, 0.192470520734787, -0.096976138651371, -0.1550595611333847, -0.037772223353385925, 0.14033402502536774, 0.12967245280742645, 0.09828595072031021, -0.030293650925159454, -0.05195188149809837, -0.15360118448734283, 0.0669139102101326, -0.085040383040905, 0.15979178249835968, 0.027525868266820908, 0.006727415136992931, -0.02795257233083248, -0.15495546162128448, -0.014495594426989555, -0.04332251474261284, -0.0643540471792221, 0.04900425300002098, 0.06198544800281525, 0.08021270483732224, 0.05572452023625374, -0.06846937537193298, 0.20453917980194092, -0.22179877758026123, 0.1848280131816864, 0.022977743297815323, 0.11162013560533524, 0.030821070075035095, 0.04245951026678085, 0.04716074466705322, 0.04580146446824074, 0.03492804616689682, 0.08078557252883911, 0.002596506616100669, 0.0808945745229721, 0.014330564998090267, -0.01238550990819931, 0.15903961658477783, 0.06658602505922318, -0.053355760872364044, 0.20289626717567444, -0.07839559763669968, 0.06523717194795609, 0.1056288406252861, 0.01860618032515049, -0.0533745214343071, -0.054855167865753174, 0.04463004320859909, 0.0032925524283200502, -0.07772934436798096, 0.07039734721183777, 0.08139403909444809, 0.16095446050167084, -0.05666501075029373, 0.003567505395039916, 0.18471990525722504, -0.11087251454591751, 0.2267712503671646, -0.06251656264066696, -0.2503175735473633, -0.0988849624991417, 0.21249909698963165, -0.06565537303686142, -0.08424890041351318, 0.016077030450105667, 0.06144425645470619, -0.15893937647342682, 0.1682390719652176, 0.03598398342728615, -0.12893563508987427, 0.06199841946363449, -0.02307078242301941, 0.00101484137121588, 0.12179797142744064, -0.06670713424682617, -0.06233029440045357, 0.04765748232603073, 0.0785781666636467, 0.20677152276039124, 0.3095415532588959, 0.06972217559814453, 0.08970940113067627, -0.0828368216753006, 0.06017475575208664, -0.08031028509140015, 0.22594550251960754, 0.09611015021800995, -0.04542055353522301, -0.09500842541456223, 0.010205503553152084, 0.06791277974843979, 0.11977081745862961, -0.09960301220417023, -0.07920767366886139, 0.020479716360569, -0.004671930801123381, 0.04756808280944824, 0.019837526604533195, 0.08810766041278839, -0.05327969789505005, 0.0056920964270830154, 0.021553026512265205, -0.12132184952497482, 0.0068231732584536076, 0.10519668459892273, -0.053794752806425095, 0.014668779447674751, 0.044748760759830475, -0.13632400333881378, 0.12686574459075928, 0.07745558023452759, 0.2579856514930725, 0.10957382619380951, -0.11055891215801239, -0.11089006811380386, -0.02301328256726265, -0.03334682434797287, 0.03526143729686737, -0.22868917882442474, 0.15004095435142517, 0.018418293446302414, -0.002352894516661763, 0.13698357343673706, -0.03523964434862137, 0.1372842788696289, 0.12579432129859924, 0.05075271055102348, -0.07371111214160919, 0.20790810883045197, 0.046990323811769485, -0.15204180777072906, -0.13524191081523895, 0.061610959470272064, 0.11962466686964035, 0.012866159901022911, -0.1373533308506012, -0.04491249471902847, -0.3805012106895447, -0.010139848105609417, 0.04093196243047714, 0.08669480681419373, -0.06263493001461029, 0.024607278406620026, -0.08836666494607925, -0.009983034804463387, 0.1520526111125946, 0.08754876255989075, 0.09044890105724335, -0.063264399766922, -0.025063971057534218, 0.11260411888360977, -0.025933127850294113, -0.050847314298152924, 0.008821716532111168, -0.08679252117872238, 0.07333794236183167, 0.04016827419400215, -0.09161416441202164, 0.039700768887996674, 0.10291055589914322, 0.08721034228801727, 0.053745392709970474, 0.030205531045794487, -0.017294391989707947, -0.07576340436935425, -0.028465058654546738, -0.05955972895026207, 0.029057571664452553, 0.08197907358407974, 0.16743704676628113, -0.12142293155193329, 0.15049925446510315, -0.0010928134433925152, -0.018105000257492065, -0.08898373693227768, -0.16220121085643768, 0.14843682944774628, 0.02046080119907856, -0.08220615983009338, 0.07959580421447754, 0.014303215779364109, -0.12575893104076385, 0.036893464624881744, 0.10826867818832397, -0.15957477688789368, -0.1894349604845047, 0.054210908710956573, 0.2119034230709076, 0.0005928873433731496, -0.1237662062048912, -0.23976564407348633, 0.009686975739896297, -0.11790481954813004, -0.14709629118442535, -0.09163988381624222, 0.06809242069721222, 0.09442295134067535, 0.07081552594900131, -0.02236175164580345, -0.04511755332350731, -0.08617918938398361, -0.05830443277955055, 0.09556983411312103, -0.17196883261203766, -0.09517918527126312, -0.18531277775764465, 0.11932706087827682, -0.11526359617710114, 0.05461481586098671, -0.13396024703979492, 0.017147429287433624, 0.11748937517404556, 0.10612718760967255, -0.15842387080192566, 0.006346470210701227, 0.10688149929046631, 0.23863846063613892, 0.06430844217538834, 0.01493169367313385, 0.06144946441054344, -0.025476112961769104, 0.1148417741060257, 0.05191541463136673, -0.1610921323299408, 0.2012902796268463, -0.09733269363641739, -0.02793458290398121, -0.15896698832511902, -0.09575945138931274, -0.2461545467376709, 0.06505025923252106, -0.0697915330529213, 0.226786807179451, -0.00978307519108057, 0.06958926469087601, -0.09479948878288269, 0.07038357853889465, -0.016404882073402405, 0.03382677584886551, 0.09160134196281433, 0.0700303390622139, 0.11864680051803589, 0.22270821034908295, -0.0495573915541172, 0.07345402240753174, -0.11557351052761078, -0.019217677414417267, 0.08861244469881058, -0.1025652289390564, 0.14574140310287476, -0.16109861433506012, 0.11556056141853333, -0.00039518249104730785, 0.043491821736097336, -0.056688763201236725, -0.07783721387386322, -0.02747523970901966, -0.10335785895586014, 0.07739511132240295, 0.10749600827693939, -0.11131533980369568, -0.22244690358638763, -0.07928254455327988, -0.0005578211857937276, -0.014758960343897343, -0.172038733959198, -0.09995470941066742, 0.25754567980766296, 0.00763398502022028, -0.0787099152803421, 0.11306112259626389, -0.0058190240524709225, -0.18224255740642548, -0.003754829056560993, 0.18437211215496063, 0.06687875092029572, -0.01728850044310093, -0.32178595662117004, -0.025655390694737434, 0.03829876706004143, 0.015285508707165718, -0.0575965978205204, 0.011977956630289555, -0.1402265578508377, -0.16472496092319489, -0.04477394372224808, -0.059034839272499084, 0.05833207070827484, 0.06993503868579865, -0.10105829685926437, 0.023300347849726677, 0.10878930240869522, -0.13589027523994446, -0.1406530737876892, 0.2529456317424774, -0.03307272493839264, -0.0026369444094598293, -0.06692609935998917, 0.12053008377552032, -0.01975869946181774, -0.019526828080415726, -0.09441936016082764, 0.08519095927476883, -0.04821939021348953, 0.19698259234428406, -0.08381053805351257, 0.03346462547779083, 0.10829389095306396, -0.2979810833930969, -0.14281530678272247, -0.007870139554142952, -0.13587471842765808, 0.04219060763716698, -0.17168043553829193, 0.05646342411637306, -0.06918243318796158, -0.21368899941444397, -0.06191045045852661, 0.08712872862815857, 0.06301678717136383, 0.06434079259634018, -0.03438898175954819, -0.07838620990514755, -0.07019641995429993, 0.23228834569454193, 0.06460542231798172, -0.13749687373638153, -0.2181399166584015, 0.14600765705108643, -0.036104802042245865, -0.22970621287822723, -0.18613559007644653, 0.0456843376159668, 0.06232161819934845, 0.1266849935054779, 0.10255847871303558, 0.015454231761395931, 0.15713746845722198, -0.09738174080848694, 0.03758948668837547, 0.1395464390516281, -0.061230070888996124, 0.11386965215206146, 0.03621440753340721, -0.06276974827051163, 0.20083735883235931, -0.005944107659161091, 0.010530581697821617, 0.23031233251094818, 0.013792221434414387, 0.01520595420151949, -0.07494179159402847, 0.0018856801325455308, 0.1172032505273819, -0.22255641222000122, 0.014490156434476376, -0.019704049453139305, 0.141381174325943, 0.16448871791362762, -0.023259829729795456, -0.17555081844329834, -0.01577446423470974, 0.043058786541223526, 0.15058492124080658, 0.0010487694526091218, -0.07055175304412842, -0.0921395793557167, 0.060562796890735626, -0.1099497526884079, 0.07404159009456635, -0.14024759829044342, -0.03399627283215523, -0.0040581077337265015, 0.09643753618001938, 0.089094378054142, -0.11344307661056519, -0.2385844886302948, 0.012196569703519344, 0.040886782109737396, -0.01369392592459917, -0.26064854860305786, 0.04346245527267456, 0.11210793256759644, 0.013039546087384224, 0.054311785846948624, 0.1590108722448349, 0.16493186354637146, 0.1484299749135971, 0.014868867583572865, 0.038939330726861954, 0.08068783581256866, 0.06566958129405975, -0.045001182705163956, -0.08727219700813293, -0.06144484505057335, 0.042172133922576904, 0.195426806807518, -0.036373138427734375, 0.04075215756893158, 0.08969924598932266, 0.18106837570667267, 0.03545723855495453, -0.16722097992897034, 0.00744110019877553, 0.1559974104166031, 0.08891075104475021, 0.07015278935432434, 0.0016141296364367008, 0.24754630029201508, 0.021262003108859062, -0.15107335150241852, -0.026820432394742966, 0.22346347570419312, 0.03229764103889465, 0.1206003800034523, 0.18432828783988953, 0.10061005502939224, 0.11456163227558136, 0.0016624167328700423, -0.06957314908504486, -0.013223309069871902, 0.10706819593906403, -0.20817610621452332, 0.13303638994693756, 0.10647177696228027, 0.09851155430078506, -0.06356440484523773, 0.12370298057794571, 0.28279781341552734, -0.007146870251744986, 0.05543262138962746, 0.13790389895439148, 0.04294396564364433, 0.042259782552719116, -0.08403877168893814, -0.023472854867577553, -0.28755927085876465, -0.0532861053943634, 0.08047324419021606, -0.08067847788333893, -0.020508896559476852, 0.10443936288356781, -0.12725508213043213, 0.2511005401611328, -0.08953447639942169, 0.07461737841367722, 0.08377204090356827, -0.016978196799755096, 0.1906147450208664, -0.07145130634307861, -0.12501400709152222, 0.0577431321144104, -0.09614608436822891, -0.13406796753406525, 0.26954954862594604, -0.15301962196826935, 0.07444219291210175, 0.014979447238147259, -0.006685775704681873, -0.11393918097019196, 0.057519927620887756, 0.052983347326517105, 0.014205126091837883, -0.03607824444770813, 0.05513892322778702, -0.09631041437387466, 0.09408313781023026, -0.1522359699010849, -0.058939751237630844, 0.1140221431851387, 0.008081882260739803, 0.2324407696723938, 0.24055765569210052, 0.12501320242881775, 0.0687321126461029, -0.21413996815681458, 0.14088693261146545, -0.07230490446090698, -0.03889323025941849, -0.09916963428258896, -0.03954743593931198, 0.11694991588592529, 0.12195683270692825, 0.1413070261478424, -0.16009855270385742, 0.08836942166090012, -0.18662068247795105, -0.032691530883312225, 0.2011212706565857, 0.08565720915794373, -0.10530595481395721, -0.11220736801624298, 0.20246738195419312, -0.08798158168792725, 0.24111677706241608, 0.13662858307361603, -0.10869943350553513, -0.17444182932376862, -0.06170869246125221, 0.07194554060697556, -0.06287267059087753, -0.08711544424295425, -0.17446577548980713, 0.07147789001464844, 0.15634816884994507, -0.0977826789021492, 0.14851757884025574, 0.21779654920101166, 0.08069766312837601, -0.08939165621995926, 0.1304578334093094, 0.010823655873537064, -0.047059543430805206, 0.063566192984581, -0.17373314499855042, -0.18230558931827545, -0.03891449049115181, 0.09240259975194931, 0.20908428728580475, -0.010427294299006462, 0.18838167190551758, -0.08394058793783188, -0.10041578114032745, -0.01964007318019867, -0.03298120200634003, 0.11247875541448593, -0.00880667194724083, -0.12191351503133774, -0.05688076838850975, -0.0014078030362725258, 0.0370081402361393, -0.13133081793785095, 0.0517362542450428, -0.0828113704919815, 0.10092980414628983, -0.1257583051919937, 0.05309382453560829, -0.022689135745167732, 0.08024328947067261, 0.08047155290842056, -0.003050302853807807, -0.08852797001600266, 0.08440793305635452, -0.09974562376737595, 0.07423921674489975, 0.02302527241408825, -0.09012943506240845, 0.1494895964860916, 0.1736101508140564, -0.006048296578228474, -0.06107030808925629, -0.010324513539671898, -0.010211403481662273, -0.004791580606251955, 0.2011958807706833, 0.11543532460927963, -0.06948532909154892, 0.011186177842319012, 0.16940481960773468, 0.14613285660743713, -0.08804640173912048, 0.15647576749324799, -0.18043860793113708, 0.052079685032367706, 0.03843161091208458, 0.14193889498710632, -0.008849560283124447, 0.009176778607070446, 0.0249634962528944, 0.20995469391345978, -0.1838628351688385, 0.048003386706113815, 0.05716926231980324, 0.12334594130516052, 0.0881102979183197, 0.03663185238838196, 0.019309882074594498, -0.26304903626441956, -0.10199450701475143, -0.06460452079772949, 0.104763925075531, 0.006609656382352114, 0.1155056282877922, 0.014103688299655914, 0.05020155757665634, -0.19972416758537292, 0.054299287497997284, -0.07270185649394989, 0.0045912726782262325, -0.010889062657952309, -0.0047149271704256535, -0.018887843936681747, 0.009208408184349537, 0.09474630653858185, 0.027422502636909485, 0.19141986966133118, -0.08568007498979568, 0.11396995931863785, -0.09679839760065079, -0.04547642916440964, 0.10005595535039902, -0.11498721688985825, 0.09608161449432373, 0.0061053023673594, -0.05966727435588837, -0.024117419496178627, -0.17420917749404907, 0.023836711421608925, -0.04658540338277817, 0.0007292012451216578, 0.13438597321510315, -0.05255692079663277, -0.0821046307682991, 0.07361787557601929, 0.11658113449811935, 0.11057507246732712, 0.03767703101038933, -0.15479212999343872, -0.03635083884000778, -0.049348920583724976, 0.09244268387556076, -0.053557101637125015, -0.11592143774032593, 0.11148814111948013, -0.1563536673784256, 0.0644056499004364, 0.010329514741897583, -0.035873740911483765, -0.017408138141036034, 0.06778961420059204, -0.04758622124791145, -0.1559455543756485, -0.21598006784915924, 0.10554824769496918, -0.1534067541360855, 0.1336556375026703, 0.07527919858694077, -0.18784655630588531, -0.10200627893209457, 0.02637345716357231, 0.0009571161936037242, -0.029741283506155014, 0.03850138932466507, 0.09336981922388077, 0.05355942249298096, 0.05996987968683243, 0.14864324033260345, -0.15909913182258606, -0.029434632509946823, -0.02636689506471157, -0.07631327956914902, -0.20670659840106964, 0.09107452630996704, 0.17815598845481873, -0.20180191099643707, 0.09133008867502213, -0.13318629562854767, -0.23955896496772766, 0.07577965408563614, -0.06232927367091179, -0.18229374289512634, -0.022875478491187096, 0.05782964453101158, 0.06963726878166199, -0.06599806249141693, 0.13307979702949524, -0.006891372613608837, -0.1662193387746811, 0.06866385787725449, 0.17253726720809937, -0.04457015544176102, 0.12260725349187851, 0.05314904823899269, -0.09083247929811478, 0.13170160353183746, -0.003885227255523205, 0.08693869411945343, -0.02446231059730053, 0.054961197078228, -0.03900114819407463, -0.1669377237558365, 0.2081482857465744, 0.17513316869735718, 0.02131003886461258, 0.1651914268732071, -0.1294715404510498, 0.03780071809887886, -0.1591847687959671, -0.017293652519583702, 0.1777212768793106, 0.21931655704975128, -0.11185634136199951, -0.017189377918839455, -0.03723751753568649, -0.08371178805828094, -0.043772224336862564, 0.04466530680656433, -0.0337226465344429, -0.03062550537288189, -0.1489565670490265, 0.10284468531608582, -0.12525884807109833, -0.06013379991054535, -0.07816287875175476, -0.16268029808998108, -0.016954783350229263, -0.1263326108455658, -0.03520713001489639, -0.03699808195233345, -0.06983144581317902, 0.18619710206985474, -0.02411721833050251, -0.01358161773532629, 0.17055398225784302, -0.08564560860395432, -0.12460801005363464, -0.030269701033830643, 0.032708946615457535, -0.06034132465720177, 0.011330909095704556, -0.07401065528392792, 0.22147801518440247, -0.008459397591650486, -0.04346415400505066, 0.1781754046678543, -0.046276431530714035, -0.17798452079296112, -0.05946485325694084, -0.24534891545772552, -0.13753388822078705, -0.19048498570919037, 0.1425948292016983, -0.019797325134277344, 0.04452170431613922, -0.1011718288064003, 0.11878751218318939, 0.21452800929546356]]}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_from_pickle(file_path):\n",
    "    with open(file_path, 'rb') as pickle_file:\n",
    "        return pickle.load(pickle_file)\n",
    "\n",
    "file_path = f'/scratch/gpfs/{princeton_id}/QCB557_project/data/H3K4me3/results_dict.pkl'\n",
    "results_dict = load_from_pickle(file_path)\n",
    "\n",
    "# Printing the first and last key content\n",
    "first_key = next(iter(results_dict))\n",
    "last_key = list(results_dict.keys())[-1]\n",
    "\n",
    "print(\"First key content:\")\n",
    "print(results_dict[first_key])\n",
    "\n",
    "print(\"\\nLast key content:\")\n",
    "print(results_dict[last_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f16bea-df80-4734-becf-402dbb02f7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna",
   "language": "python",
   "name": "dna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
