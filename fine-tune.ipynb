{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60a65e5-4d03-41a4-9fe3-4731012cad99",
   "metadata": {},
   "source": [
    "# Fine-tuning script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d251927-6ec1-4a38-ae01-5cfb2b45fb55",
   "metadata": {},
   "source": [
    "*Anusha, Joyce, Nina*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459bf7cb-8cb3-49b8-9d1c-98ff46686ffd",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e6f60bbc-a2e8-4ad3-acdf-0ceba0a38fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    AutoModelForSequenceClassification, \n",
    "    DataCollatorForTokenClassification, \n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from transformers.models.bert.configuration_bert import BertConfig \n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "#import Dataset\n",
    "\n",
    "import csv\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d38251-2066-40f6-84f3-c4412cba2e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ids and comment out others when using\n",
    "princeton_id = 'aa8417'\n",
    "#princeton_id = 'ns...'\n",
    "#princeton_id = 'jf...'\n",
    "\n",
    "project_dir = f'/scratch/gpfs/{princeton_id}/QCB557_project'\n",
    "\n",
    "model_name = 'fine_tune_v1'\n",
    "model_out_dir = f'{project_dir}/models/{model_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89162e24-9c9d-4777-8310-3927638dacce",
   "metadata": {},
   "source": [
    "#### Load model and tokenizer from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9826f6a-6d87-47ee-8778-d9700397329a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# use gpu\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\")\n",
    "config.num_labels #2 labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, config=config)\n",
    "\n",
    "# don't need to move the tokenizer to gpu b/c it's light\n",
    "# use data collator to pad sequences dynamically during training\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, padding=True)\n",
    "tokenizer.pad_token = \"X\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804470ee-28a9-4a7b-a8a5-e72651dfcb52",
   "metadata": {
    "tags": []
   },
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f4262-11eb-4aae-81f2-62c264d798ea",
   "metadata": {},
   "source": [
    "#### Freeze and unfreeze gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a742af6-b655-4584-8605-62d74da0416f",
   "metadata": {},
   "source": [
    "We can play around with this later as we imrove the performance of the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0f61d91-4d77-479c-b3e3-b622d5808ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default when using AutoModelForSequenceClassification is that all pretrained parameters/layers are frozen except classification head\n",
    "# as we go through rounds of fine-tuning, we can optionally unfreeze from of the pretrained layers to improve performance\n",
    "\n",
    "# unfreeze the last layer in the encoder block\n",
    "for name, param in model.named_parameters():\n",
    "    if \"encoder.layer.11\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9330e2f3-e3ab-4772-a02c-1ee2100b73ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f4bca-6dc8-4898-a2b7-9930fc378101",
   "metadata": {},
   "source": [
    "#### Load data from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b9a8ad43-cd08-45f6-8ff8-e6698343d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_data_load(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, shuffle=True):\n",
    "        if shuffle:\n",
    "            self.dataframe = dataframe.sample(frac=1).reset_index(drop=True)  # shuffle the dataframe\n",
    "        else:\n",
    "            self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.dataframe.iloc[idx]['sequence']\n",
    "        label = (self.dataframe.iloc[idx]['label'])\n",
    "\n",
    "        # tokenize the sequence\n",
    "        # tokenizer automatically generates attention masks\n",
    "        #inputs = self.tokenizer(sequence, padding='max_length', max_length=500, truncation=True, return_tensors='pt')\n",
    "        inputs = self.tokenizer(sequence, padding='max_length', max_length=500, return_tensors='pt')\n",
    "\n",
    "        \n",
    "        # move inputs to gpu\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': (torch.tensor([label], dtype=torch.long)).to(device)\n",
    "            #'labels': torch.tensor([int(label)]) \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da922b8-534d-4773-a9fe-ac32e9b9eb32",
   "metadata": {},
   "source": [
    "#### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc6ed7f8-af00-4d7d-b63f-3c9f7fcf2a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_to_split = pd.read_csv(f'{project_dir}/data/train.csv')\n",
    "train_data_to_split['label'] = train_data_to_split['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "83c0ca0d-f531-435e-bfcc-a46d801cd68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(f'{project_dir}/data/test.csv')\n",
    "test_data['label'] = test_data['label'].astype(int)\n",
    "\n",
    "test_ds = custom_data_load(dataframe = test_data, tokenizer = tokenizer, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfda9b5-0eaa-402c-986f-f816eb07faa5",
   "metadata": {},
   "source": [
    "#### Split training into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "37dc3169-165e-47f9-a278-ab2de4ac3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_test_split(train_data_to_split, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8f0d4b7e-2a6b-4b07-b7be-43e3eedc27da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = custom_data_load(dataframe = train_data, tokenizer = tokenizer, shuffle=True)\n",
    "valid_ds = custom_data_load(dataframe = valid_data, tokenizer = tokenizer, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9632a30-e60f-4079-87df-e05b8902baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataLoader(train_ds, pin_memory=True)\n",
    "valid = DataLoader(valid_ds, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec92215c-4c80-4c3d-8421-f92cd5702014",
   "metadata": {},
   "source": [
    "#### Data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7485901b-5614-4d4b-a668-74e6faf88aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer, label_pad_token_id='X')\n",
    "#data_collator = DataCollatorWithPadding(tokenizer)\n",
    "#data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da355e56-064b-4625-ba54-1aeaa748be55",
   "metadata": {},
   "source": [
    "#### Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8b8aa17d-a48b-47bb-a685-413713e56bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = model_out_dir,\n",
    "    num_train_epochs= 10, \n",
    "    per_device_train_batch_size=8, # powers of 2\n",
    "    weight_decay=0.015, # regularization\n",
    "    learning_rate=1e-5,\n",
    "    gradient_accumulation_steps=2, # how many batches of gradients are accumulated before parameter update\n",
    "    #gradient_checkpointing=True, # helps reduce memory\n",
    "    #dataloader_num_workers=4,\n",
    "    \n",
    "    log_level=\"error\",\n",
    "    evaluation_strategy=\"steps\",  \n",
    "    eval_steps=500,        \n",
    "    logging_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    save_strategy=\"no\", # don't want to save checkpoints -- takes up too much space, can change later\n",
    "    fp16=True,\n",
    "    dataloader_pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242bb20-c06f-4a4a-8bcd-d10a9f05194a",
   "metadata": {},
   "source": [
    "#### Extra metrics to compute with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d3fd445-39f5-48e9-9834-a7d32e194b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    \n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd838666-2126-4080-a63c-849c2d97af5b",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "283f8d8e-dea0-4a9e-b72c-c1aff50508c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aa8417/.conda/envs/torch-env/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 2 named labels expected length 500 but got length 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# need to fix the bugs here -- thought I already fixed this???\u001b[39;00m\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      5\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#data_collator = data_collator\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(training_args\u001b[38;5;241m.\u001b[39moutput_dir)\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.11/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.11/site-packages/transformers/trainer.py:1821\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1818\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1820\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1821\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1824\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.11/site-packages/accelerate/data_loader.py:452\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[86], line 25\u001b[0m, in \u001b[0;36mcustom_data_load.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# move inputs to gpu\u001b[39;00m\n\u001b[1;32m     23\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#'labels': torch.tensor([int(label)]) \u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.11/site-packages/datasets/arrow_dataset.py:912\u001b[0m, in \u001b[0;36mDataset.from_dict\u001b[0;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[1;32m    910\u001b[0m     arrow_typed_mapping[col] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    911\u001b[0m mapping \u001b[38;5;241m=\u001b[39m arrow_typed_mapping\n\u001b[0;32m--> 912\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m \u001b[43mInMemoryTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m     info \u001b[38;5;241m=\u001b[39m DatasetInfo()\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.11/site-packages/datasets/table.py:758\u001b[0m, in \u001b[0;36mInMemoryTable.from_pydict\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pydict\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    744\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;124;03m    Construct a Table from Arrow arrays or columns.\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;124;03m        `datasets.table.Table`\u001b[39;00m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.11/site-packages/pyarrow/table.pxi:1813\u001b[0m, in \u001b[0;36mpyarrow.lib._Tabular.from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.11/site-packages/pyarrow/table.pxi:5348\u001b[0m, in \u001b[0;36mpyarrow.lib._from_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.11/site-packages/pyarrow/table.pxi:3991\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.11/site-packages/pyarrow/table.pxi:3271\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.11/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Column 2 named labels expected length 500 but got length 1"
     ]
    }
   ],
   "source": [
    "# need to fix the bugs here -- thought I already fixed this???\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    compute_metrics=compute_metrics\n",
    "    #data_collator = data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccecfa9-e5d8-4f76-a2bd-72bf6271cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.DataFrame(trainer.state.log_history)\n",
    "log_df.to_csv(f'{project_dir}/model_output/log_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb1cdc-fc1d-4248-b625-b96a3b2603b0",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba114f-79b7-48ab-9036-35086a009c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to fix this part\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# go through test dataset, make predictions and store them\n",
    "for idx, sample in enumerate(test_ds):\n",
    "    with torch.no_grad():\n",
    "        input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "        attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        predicted_probs = torch.softmax(outputs.logits, dim=-1) # might not need this part\n",
    "        predicted_labels = torch.argmax(predicted_probs, dim=-1)\n",
    "\n",
    "        predictions.append(predicted_labels)\n",
    "        true_labels.append(sample['labels'].item())\n",
    "\n",
    "# concatenate predictions and convert true_labels to numpy array\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# save evaluation results to CSV\n",
    "results_df = pd.DataFrame({'true_labels': true_labels, 'predicted_labels': predictions})\n",
    "results_df.to_csv(f'{project_dir}/model_output/results_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4fcbf1a-9b93-4f1a-952e-498f922a04dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[2100]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "193f4cff-5745-4e0c-a85a-ce1e6d3514b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,  349, 1054,  197,   26,  987,  914,   66,  380,   43,   50, 1480,\n",
       "         153, 1396,  355,   61, 1711,  245, 1711,  245,  256,   21, 3035, 1278,\n",
       "         998,   17,  313,  276,  245,   43, 1109, 2473,  737,  108,  166,   47,\n",
       "         541,   75,   56,  800,   21,  390,   60,   32,  132, 1120,   63,  158,\n",
       "          80,  286,  852, 2142, 2789,  243,  303,  194,   17,  397,   97,   45,\n",
       "        3213,  502,   53,   58,  358, 3444,  160,  602,   35,  299,   59, 1925,\n",
       "          29,  552,  179, 1183,   76,  276,  153,  927,   78,  196,  145,   28,\n",
       "         710,   56,   67,   55,  970,  142,  969, 1340, 2546, 1538,  116, 3884,\n",
       "        2006,   47,    8,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[2000]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f92eb2-43ee-4123-8be1-6506b68c4c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
