{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60a65e5-4d03-41a4-9fe3-4731012cad99",
   "metadata": {},
   "source": [
    "# Fine-tuning script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d251927-6ec1-4a38-ae01-5cfb2b45fb55",
   "metadata": {},
   "source": [
    "*Anusha, Joyce, Nina*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459bf7cb-8cb3-49b8-9d1c-98ff46686ffd",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6f60bbc-a2e8-4ad3-acdf-0ceba0a38fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    AutoModelForSequenceClassification, \n",
    "    DataCollatorForTokenClassification, \n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from transformers.models.bert.configuration_bert import BertConfig \n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "#import Dataset\n",
    "import os\n",
    "\n",
    "import csv\n",
    "from Bio import SeqIO\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "#os.environ['WANDB_NOTEBOOK_NAME'] = 'fine-tune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d38251-2066-40f6-84f3-c4412cba2e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ids and comment out others when using\n",
    "#princeton_id = 'aa8417'\n",
    "princeton_id = 'ns5404'\n",
    "#princeton_id = 'jf...'\n",
    "\n",
    "project_dir = f'/scratch/gpfs/{princeton_id}/QCB557_project'\n",
    "\n",
    "model_name = 'fine_tune_v2'\n",
    "model_out_dir = f'{project_dir}/models/{model_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89162e24-9c9d-4777-8310-3927638dacce",
   "metadata": {},
   "source": [
    "#### Load model and tokenizer from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9826f6a-6d87-47ee-8778-d9700397329a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['classifier.weight', 'classifier.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# use gpu\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\")\n",
    "print(config.num_labels) #2 labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, config=config)\n",
    "\n",
    "# don't need to move the tokenizer to gpu b/c it's light\n",
    "# use data collator to pad sequences dynamically during training\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, padding=True)\n",
    "tokenizer.pad_token = \"X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c93b0f0a-c858-4de5-aa0c-3146def36166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804470ee-28a9-4a7b-a8a5-e72651dfcb52",
   "metadata": {
    "tags": []
   },
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f4262-11eb-4aae-81f2-62c264d798ea",
   "metadata": {},
   "source": [
    "#### Freeze and unfreeze gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a742af6-b655-4584-8605-62d74da0416f",
   "metadata": {},
   "source": [
    "We can play around with this later as we imrove the performance of the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4214e5b3-e2f8-4d6f-8208-4a5bf7ced1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a80bd-0a3e-484e-9e57-8b62d67fd5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f61d91-4d77-479c-b3e3-b622d5808ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default when using AutoModelForSequenceClassification is that all pretrained parameters/layers are frozen except classification head\n",
    "# as we go through rounds of fine-tuning, we can optionally unfreeze from of the pretrained layers to improve performance\n",
    "\n",
    "# unfreeze the last layer in the encoder block\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "440fd917-83c6-4f0e-ace5-62fec1c109eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters with gradient enabled:\n",
      "classifier.weight\n",
      "classifier.bias\n",
      "\n",
      "Parameters with gradient disabled:\n",
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.Wqkv.weight\n",
      "bert.encoder.layer.0.attention.self.Wqkv.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.mlp.gated_layers.weight\n",
      "bert.encoder.layer.0.mlp.wo.weight\n",
      "bert.encoder.layer.0.mlp.wo.bias\n",
      "bert.encoder.layer.0.mlp.layernorm.weight\n",
      "bert.encoder.layer.0.mlp.layernorm.bias\n",
      "bert.encoder.layer.1.attention.self.Wqkv.weight\n",
      "bert.encoder.layer.1.attention.self.Wqkv.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.mlp.gated_layers.weight\n",
      "bert.encoder.layer.1.mlp.wo.weight\n",
      "bert.encoder.layer.1.mlp.wo.bias\n",
      "bert.encoder.layer.1.mlp.layernorm.weight\n",
      "bert.encoder.layer.1.mlp.layernorm.bias\n",
      "bert.encoder.layer.2.attention.self.Wqkv.weight\n",
      "bert.encoder.layer.2.attention.self.Wqkv.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.mlp.gated_layers.weight\n",
      "bert.encoder.layer.2.mlp.wo.weight\n",
      "bert.encoder.layer.2.mlp.wo.bias\n",
      "bert.encoder.layer.2.mlp.layernorm.weight\n",
      "bert.encoder.layer.2.mlp.layernorm.bias\n",
      "bert.encoder.layer.3.attention.self.Wqkv.weight\n",
      "bert.encoder.layer.3.attention.self.Wqkv.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.mlp.gated_layers.weight\n",
      "bert.encoder.layer.3.mlp.wo.weight\n",
      "bert.encoder.layer.3.mlp.wo.bias\n",
      "bert.encoder.layer.3.mlp.layernorm.weight\n",
      "bert.encoder.layer.3.mlp.layernorm.bias\n",
      "bert.encoder.layer.4.attention.self.Wqkv.weight\n",
      "bert.encoder.layer.4.attention.self.Wqkv.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.mlp.gated_layers.weight\n",
      "bert.encoder.layer.4.mlp.wo.weight\n",
      "bert.encoder.layer.4.mlp.wo.bias\n",
      "bert.encoder.layer.4.mlp.layernorm.weight\n",
      "bert.encoder.layer.4.mlp.layernorm.bias\n",
      "bert.encoder.layer.5.attention.self.Wqkv.weight\n",
      "bert.encoder.layer.5.attention.self.Wqkv.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.mlp.gated_layers.weight\n",
      "bert.encoder.layer.5.mlp.wo.weight\n",
      "bert.encoder.layer.5.mlp.wo.bias\n",
      "bert.encoder.layer.5.mlp.layernorm.weight\n",
      "bert.encoder.layer.5.mlp.layernorm.bias\n",
      "bert.encoder.layer.6.attention.self.Wqkv.weight\n",
      "bert.encoder.layer.6.attention.self.Wqkv.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.mlp.gated_layers.weight\n",
      "bert.encoder.layer.6.mlp.wo.weight\n",
      "bert.encoder.layer.6.mlp.wo.bias\n",
      "bert.encoder.layer.6.mlp.layernorm.weight\n",
      "bert.encoder.layer.6.mlp.layernorm.bias\n",
      "bert.encoder.layer.7.attention.self.Wqkv.weight\n",
      "bert.encoder.layer.7.attention.self.Wqkv.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.mlp.gated_layers.weight\n",
      "bert.encoder.layer.7.mlp.wo.weight\n",
      "bert.encoder.layer.7.mlp.wo.bias\n",
      "bert.encoder.layer.7.mlp.layernorm.weight\n",
      "bert.encoder.layer.7.mlp.layernorm.bias\n",
      "bert.encoder.layer.8.attention.self.Wqkv.weight\n",
      "bert.encoder.layer.8.attention.self.Wqkv.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.mlp.gated_layers.weight\n",
      "bert.encoder.layer.8.mlp.wo.weight\n",
      "bert.encoder.layer.8.mlp.wo.bias\n",
      "bert.encoder.layer.8.mlp.layernorm.weight\n",
      "bert.encoder.layer.8.mlp.layernorm.bias\n",
      "bert.encoder.layer.9.attention.self.Wqkv.weight\n",
      "bert.encoder.layer.9.attention.self.Wqkv.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.mlp.gated_layers.weight\n",
      "bert.encoder.layer.9.mlp.wo.weight\n",
      "bert.encoder.layer.9.mlp.wo.bias\n",
      "bert.encoder.layer.9.mlp.layernorm.weight\n",
      "bert.encoder.layer.9.mlp.layernorm.bias\n",
      "bert.encoder.layer.10.attention.self.Wqkv.weight\n",
      "bert.encoder.layer.10.attention.self.Wqkv.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.mlp.gated_layers.weight\n",
      "bert.encoder.layer.10.mlp.wo.weight\n",
      "bert.encoder.layer.10.mlp.wo.bias\n",
      "bert.encoder.layer.10.mlp.layernorm.weight\n",
      "bert.encoder.layer.10.mlp.layernorm.bias\n",
      "bert.encoder.layer.11.attention.self.Wqkv.weight\n",
      "bert.encoder.layer.11.attention.self.Wqkv.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.mlp.gated_layers.weight\n",
      "bert.encoder.layer.11.mlp.wo.weight\n",
      "bert.encoder.layer.11.mlp.wo.bias\n",
      "bert.encoder.layer.11.mlp.layernorm.weight\n",
      "bert.encoder.layer.11.mlp.layernorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "#confirm classification head is frozen\n",
    "\n",
    "print(\"Parameters with gradient enabled:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "print(\"\\nParameters with gradient disabled:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9330e2f3-e3ab-4772-a02c-1ee2100b73ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f4bca-6dc8-4898-a2b7-9930fc378101",
   "metadata": {},
   "source": [
    "#### Load data from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9a8ad43-cd08-45f6-8ff8-e6698343d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_data_load(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, shuffle=True):\n",
    "        if shuffle:\n",
    "            self.dataframe = dataframe.sample(frac=1).reset_index(drop=True)  # shuffle the dataframe\n",
    "        else:\n",
    "            self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.dataframe.iloc[idx]['sequence']\n",
    "        #print(sequence)\n",
    "        #print(len(sequence))\n",
    "        label = (self.dataframe.iloc[idx]['label'])\n",
    "        #print(label)\n",
    "\n",
    "        # tokenize the sequence\n",
    "        # tokenizer automatically generates attention masks\n",
    "        #inputs = self.tokenizer(sequence, padding='max_length', max_length=500, truncation=True, return_tensors='pt')\n",
    "        inputs = self.tokenizer(sequence, padding='max_length', max_length=128, return_tensors='pt')\n",
    "        #print(inputs)\n",
    "        \n",
    "        # move inputs to gpu\n",
    "        #inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': (torch.tensor([label], dtype=torch.long))\n",
    "            #'labels': torch.tensor([int(label)]) \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da922b8-534d-4773-a9fe-ac32e9b9eb32",
   "metadata": {},
   "source": [
    "#### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc6ed7f8-af00-4d7d-b63f-3c9f7fcf2a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_to_split = pd.read_csv(f'{project_dir}/data/train.csv')\n",
    "train_data_to_split['label'] = train_data_to_split['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c0ca0d-f531-435e-bfcc-a46d801cd68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(f'{project_dir}/data/test.csv')\n",
    "test_data['label'] = test_data['label'].astype(int)\n",
    "\n",
    "test_ds = custom_data_load(dataframe = test_data, tokenizer = tokenizer, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfda9b5-0eaa-402c-986f-f816eb07faa5",
   "metadata": {},
   "source": [
    "#### Split training into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37dc3169-165e-47f9-a278-ab2de4ac3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_test_split(train_data_to_split, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f0d4b7e-2a6b-4b07-b7be-43e3eedc27da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = custom_data_load(dataframe = train_data, tokenizer = tokenizer, shuffle=True)\n",
    "valid_ds = custom_data_load(dataframe = valid_data, tokenizer = tokenizer, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9632a30-e60f-4079-87df-e05b8902baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataLoader(train_ds, pin_memory=True)\n",
    "valid = DataLoader(valid_ds, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec92215c-4c80-4c3d-8421-f92cd5702014",
   "metadata": {},
   "source": [
    "#### Data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7485901b-5614-4d4b-a668-74e6faf88aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer, label_pad_token_id='X')\n",
    "#data_collator = DataCollatorWithPadding(tokenizer)\n",
    "#data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da355e56-064b-4625-ba54-1aeaa748be55",
   "metadata": {},
   "source": [
    "#### Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b8aa17d-a48b-47bb-a685-413713e56bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = model_out_dir,\n",
    "    num_train_epochs= 3, \n",
    "    per_device_train_batch_size=32, # powers of 2\n",
    "    weight_decay=0.015, # regularization\n",
    "    learning_rate=1e-5,\n",
    "    gradient_accumulation_steps=2, # how many batches of gradients are accumulated before parameter update\n",
    "    #gradient_checkpointing=True, # helps reduce memory\n",
    "    #dataloader_num_workers=4,\n",
    "    \n",
    "    log_level=\"error\",\n",
    "    evaluation_strategy=\"epoch\",  \n",
    "    #eval_steps=500,        \n",
    "    #logging_steps=500,\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"no\", # don't want to save checkpoints -- takes up too much space, can change later\n",
    "    fp16=True,\n",
    "    #dataloader_pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242bb20-c06f-4a4a-8bcd-d10a9f05194a",
   "metadata": {},
   "source": [
    "#### Extra metrics to compute with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d3fd445-39f5-48e9-9834-a7d32e194b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to fix this, doesn't work with current prediction + label format\n",
    "def compute_metrics(eval_pred):\n",
    "    logits,labels = eval_pred\n",
    "    logits = torch.FloatTensor(logits[0])\n",
    "\n",
    "    predicted_probs = torch.softmax(logits, dim=-1) # might not need this part\n",
    "    predicted_labels = torch.argmax(predicted_probs, dim=-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predicted_labels)\n",
    "    precision = precision_score(labels, predicted_labels)\n",
    "    recall = recall_score(labels, predicted_labels)\n",
    "    f1 = f1_score(labels, predicted_labels)\n",
    "    \n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd838666-2126-4080-a63c-849c2d97af5b",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "283f8d8e-dea0-4a9e-b72c-c1aff50508c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ns5404/.conda/envs/dna/lib/python3.8/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6808, 'learning_rate': 6.666666666666667e-06, 'epoch': 1.0}\n",
      "{'eval_loss': 0.6882871985435486, 'eval_accuracy': 0.5392512077294686, 'eval_precision': 0.5377201728148887, 'eval_recall': 0.9229891614375356, 'eval_f1': 0.6795464090718185, 'eval_runtime': 13.3029, 'eval_samples_per_second': 248.968, 'eval_steps_per_second': 31.121, 'epoch': 1.0}\n",
      "{'loss': 0.6801, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.0}\n",
      "{'eval_loss': 0.6880368590354919, 'eval_accuracy': 0.5389492753623188, 'eval_precision': 0.5376917945296864, 'eval_recall': 0.9195664575014262, 'eval_f1': 0.6785939802146916, 'eval_runtime': 13.2505, 'eval_samples_per_second': 249.953, 'eval_steps_per_second': 31.244, 'epoch': 2.0}\n",
      "{'loss': 0.6801, 'learning_rate': 0.0, 'epoch': 3.0}\n",
      "{'eval_loss': 0.6879181265830994, 'eval_accuracy': 0.5395531400966184, 'eval_precision': 0.5380507343124166, 'eval_recall': 0.9195664575014262, 'eval_f1': 0.678879764160876, 'eval_runtime': 13.2835, 'eval_samples_per_second': 249.332, 'eval_steps_per_second': 31.167, 'epoch': 3.0}\n",
      "{'train_runtime': 257.7267, 'train_samples_per_second': 346.961, 'train_steps_per_second': 5.424, 'train_loss': 0.6803261643656675, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    compute_metrics=compute_metrics\n",
    "    #data_collator = data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19742b20-14ba-462f-afc3-eca6246cd360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ccecfa9-e5d8-4f76-a2bd-72bf6271cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.DataFrame(trainer.state.log_history)\n",
    "log_df.to_csv(f'{project_dir}/model_output/log_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e4aebea5-f5e3-4b32-8500-27d5ef294377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>466</td>\n",
       "      <td>0.688287</td>\n",
       "      <td>0.539251</td>\n",
       "      <td>0.537720</td>\n",
       "      <td>0.922989</td>\n",
       "      <td>0.679546</td>\n",
       "      <td>13.3029</td>\n",
       "      <td>248.968</td>\n",
       "      <td>31.121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6801</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.0</td>\n",
       "      <td>932</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>932</td>\n",
       "      <td>0.688037</td>\n",
       "      <td>0.538949</td>\n",
       "      <td>0.537692</td>\n",
       "      <td>0.919566</td>\n",
       "      <td>0.678594</td>\n",
       "      <td>13.2505</td>\n",
       "      <td>249.953</td>\n",
       "      <td>31.244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1398</td>\n",
       "      <td>0.687918</td>\n",
       "      <td>0.539553</td>\n",
       "      <td>0.538051</td>\n",
       "      <td>0.919566</td>\n",
       "      <td>0.678880</td>\n",
       "      <td>13.2835</td>\n",
       "      <td>249.332</td>\n",
       "      <td>31.167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257.7267</td>\n",
       "      <td>346.961</td>\n",
       "      <td>5.424</td>\n",
       "      <td>7.823687e+15</td>\n",
       "      <td>0.680326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss  learning_rate  epoch  step  eval_loss  eval_accuracy  \\\n",
       "0  0.6808       0.000007    1.0   466        NaN            NaN   \n",
       "1     NaN            NaN    1.0   466   0.688287       0.539251   \n",
       "2  0.6801       0.000003    2.0   932        NaN            NaN   \n",
       "3     NaN            NaN    2.0   932   0.688037       0.538949   \n",
       "4  0.6801       0.000000    3.0  1398        NaN            NaN   \n",
       "5     NaN            NaN    3.0  1398   0.687918       0.539553   \n",
       "6     NaN            NaN    3.0  1398        NaN            NaN   \n",
       "\n",
       "   eval_precision  eval_recall   eval_f1  eval_runtime  \\\n",
       "0             NaN          NaN       NaN           NaN   \n",
       "1        0.537720     0.922989  0.679546       13.3029   \n",
       "2             NaN          NaN       NaN           NaN   \n",
       "3        0.537692     0.919566  0.678594       13.2505   \n",
       "4             NaN          NaN       NaN           NaN   \n",
       "5        0.538051     0.919566  0.678880       13.2835   \n",
       "6             NaN          NaN       NaN           NaN   \n",
       "\n",
       "   eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "0                      NaN                    NaN            NaN   \n",
       "1                  248.968                 31.121            NaN   \n",
       "2                      NaN                    NaN            NaN   \n",
       "3                  249.953                 31.244            NaN   \n",
       "4                      NaN                    NaN            NaN   \n",
       "5                  249.332                 31.167            NaN   \n",
       "6                      NaN                    NaN       257.7267   \n",
       "\n",
       "   train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n",
       "0                       NaN                     NaN           NaN         NaN  \n",
       "1                       NaN                     NaN           NaN         NaN  \n",
       "2                       NaN                     NaN           NaN         NaN  \n",
       "3                       NaN                     NaN           NaN         NaN  \n",
       "4                       NaN                     NaN           NaN         NaN  \n",
       "5                       NaN                     NaN           NaN         NaN  \n",
       "6                   346.961                   5.424  7.823687e+15    0.680326  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc56cf37-fba2-4539-901a-840d0e6d2a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8001d63-5a99-4523-a084-7fb9f37c823d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6808</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>466</td>\n",
       "      <td>0.688287</td>\n",
       "      <td>0.539251</td>\n",
       "      <td>0.537720</td>\n",
       "      <td>0.922989</td>\n",
       "      <td>0.679546</td>\n",
       "      <td>13.3029</td>\n",
       "      <td>248.968</td>\n",
       "      <td>31.121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6801</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.0</td>\n",
       "      <td>932</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>932</td>\n",
       "      <td>0.688037</td>\n",
       "      <td>0.538949</td>\n",
       "      <td>0.537692</td>\n",
       "      <td>0.919566</td>\n",
       "      <td>0.678594</td>\n",
       "      <td>13.2505</td>\n",
       "      <td>249.953</td>\n",
       "      <td>31.244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1398</td>\n",
       "      <td>0.687918</td>\n",
       "      <td>0.539553</td>\n",
       "      <td>0.538051</td>\n",
       "      <td>0.919566</td>\n",
       "      <td>0.678880</td>\n",
       "      <td>13.2835</td>\n",
       "      <td>249.332</td>\n",
       "      <td>31.167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257.7267</td>\n",
       "      <td>346.961</td>\n",
       "      <td>5.424</td>\n",
       "      <td>7.823687e+15</td>\n",
       "      <td>0.680326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     loss  learning_rate  epoch  step  eval_loss  eval_accuracy  \\\n",
       "0  0.6808       0.000007    1.0   466        NaN            NaN   \n",
       "1     NaN            NaN    1.0   466   0.688287       0.539251   \n",
       "2  0.6801       0.000003    2.0   932        NaN            NaN   \n",
       "3     NaN            NaN    2.0   932   0.688037       0.538949   \n",
       "4  0.6801       0.000000    3.0  1398        NaN            NaN   \n",
       "5     NaN            NaN    3.0  1398   0.687918       0.539553   \n",
       "6     NaN            NaN    3.0  1398        NaN            NaN   \n",
       "\n",
       "   eval_precision  eval_recall   eval_f1  eval_runtime  \\\n",
       "0             NaN          NaN       NaN           NaN   \n",
       "1        0.537720     0.922989  0.679546       13.3029   \n",
       "2             NaN          NaN       NaN           NaN   \n",
       "3        0.537692     0.919566  0.678594       13.2505   \n",
       "4             NaN          NaN       NaN           NaN   \n",
       "5        0.538051     0.919566  0.678880       13.2835   \n",
       "6             NaN          NaN       NaN           NaN   \n",
       "\n",
       "   eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "0                      NaN                    NaN            NaN   \n",
       "1                  248.968                 31.121            NaN   \n",
       "2                      NaN                    NaN            NaN   \n",
       "3                  249.953                 31.244            NaN   \n",
       "4                      NaN                    NaN            NaN   \n",
       "5                  249.332                 31.167            NaN   \n",
       "6                      NaN                    NaN       257.7267   \n",
       "\n",
       "   train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n",
       "0                       NaN                     NaN           NaN         NaN  \n",
       "1                       NaN                     NaN           NaN         NaN  \n",
       "2                       NaN                     NaN           NaN         NaN  \n",
       "3                       NaN                     NaN           NaN         NaN  \n",
       "4                       NaN                     NaN           NaN         NaN  \n",
       "5                       NaN                     NaN           NaN         NaN  \n",
       "6                   346.961                   5.424  7.823687e+15    0.680326  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e773d45-2139-46d2-a8f4-91374079f5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.6808\n",
       "1       NaN\n",
       "2    0.6801\n",
       "3       NaN\n",
       "4    0.6801\n",
       "5       NaN\n",
       "6       NaN\n",
       "Name: loss, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9c22190-648c-4a0a-a79f-520182efa6b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m train \u001b[38;5;241m=\u001b[39m log_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m validation \u001b[38;5;241m=\u001b[39m log_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epoch, validation, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/dna/lib/python3.8/site-packages/matplotlib/pyplot.py:2761\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2759\u001b[0m \u001b[38;5;129m@docstring\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   2760\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2762\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m   2763\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dna/lib/python3.8/site-packages/matplotlib/axes/_axes.py:1647\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1646\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1647\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/.conda/envs/dna/lib/python3.8/site-packages/matplotlib/axes/_base.py:216\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    215\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dna/lib/python3.8/site-packages/matplotlib/axes/_base.py:331\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m         kw[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tup) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43m_check_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     y \u001b[38;5;241m=\u001b[39m _check_1d(tup[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/dna/lib/python3.8/site-packages/matplotlib/cbook/__init__.py:1349\u001b[0m, in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m w:\n\u001b[1;32m   1344\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\n\u001b[1;32m   1345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malways\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1346\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m   1347\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSupport for multi-dimensional indexing\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1349\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mndim\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;66;03m# we have definitely hit a pandas index or series object\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;66;03m# cast to a numpy array.\u001b[39;00m\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(w) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/dna/lib/python3.8/site-packages/pandas/core/series.py:1033\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values(key)\n\u001b[0;32m-> 1033\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/dna/lib/python3.8/site-packages/pandas/core/series.py:1048\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexing a Series with DataFrame is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported, use the appropriate DataFrame column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m-> 1048\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_values_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key):\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# e.g. scalars that aren't recognized by lib.is_scalar, GH#32684\u001b[39;00m\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc[key]\n",
      "File \u001b[0;32m~/.conda/envs/dna/lib/python3.8/site-packages/pandas/core/series.py:1082\u001b[0m, in \u001b[0;36mSeries._get_values_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39many_none(\u001b[38;5;241m*\u001b[39mkey):\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# mpl compat if we look up e.g. ser[:, np.newaxis];\u001b[39;00m\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;66;03m#  see tests.series.timeseries.test_mpl_compat_hack\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;66;03m# the asarray is needed to avoid returning a 2D DatetimeArray\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key])\n\u001b[0;32m-> 1082\u001b[0m     \u001b[43mdisallow_ndim_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n",
      "File \u001b[0;32m~/.conda/envs/dna/lib/python3.8/site-packages/pandas/core/indexers/utils.py:343\u001b[0m, in \u001b[0;36mdisallow_ndim_indexing\u001b[0;34m(result)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mHelper function to disallow multi-dimensional indexing on 1D Series/Index.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03min GH#30588.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(result) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMulti-dimensional indexing (e.g. `obj[:, None]`) is no longer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported. Convert to a numpy array before indexing instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read the log\n",
    "log_df = pd.read_csv(f'{project_dir}/model_output/log_{model_name}.csv')\n",
    "\n",
    "# plot the loss curve\n",
    "epoch = log_df['epoch']\n",
    "train = log_df['loss']\n",
    "validation = log_df['eval_loss']\n",
    "\n",
    "plt.plot(epoch, train, label='training loss')\n",
    "plt.plot(epoch, validation, label='validation loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('training and validation loss curve')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891d7c6-f00d-4bbb-8c0e-9df0bf159c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the log\n",
    "log_df = pd.read_csv(f'{project_dir}/model_output/log_{model_name}.csv')\n",
    "\n",
    "# plot the loss curve\n",
    "epoch = log_df['epoch']\n",
    "#train_acc = log_df['accuracy']\n",
    "valid_acc = log_df['eval_accuracy']\n",
    "\n",
    "#plt.plot(epoch, train_acc, label='training accuracy')\n",
    "plt.plot(epoch, valid_acc, label='validation accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('training and validation accuracy curves')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a7897-6f77-4752-bc15-286a33728d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the log\n",
    "log_df = pd.read_csv(f'{project_dir}/model_output/log_{model_name}.csv')\n",
    "\n",
    "# plot the loss curve\n",
    "epoch = log_df['epoch']\n",
    "valid_prec = log_df['precision']\n",
    "valid_rec = log_df['recall']\n",
    "\n",
    "plt.plot(epoch, valid_prec, label='precision')\n",
    "plt.plot(epoch, valid_rec, label='recall')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('precision/recall')\n",
    "plt.title('precision and recall during validation')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e963e-529d-4e32-a28c-4ce97cf20d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa168a-4395-457d-9e53-d87750af45cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd771ca-635f-42fe-a367-d1851502ceff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25eb1cdc-fc1d-4248-b625-b96a3b2603b0",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac0d94e4-711d-43d0-b53e-5c928b8753af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load fine-tuned model\n",
    "\n",
    "config = BertConfig.from_pretrained(f'{training_args.output_dir}/config.json')\n",
    "print(config.num_labels) #2 labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(training_args.output_dir, trust_remote_code=True, config=config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f4ed5d87-9785-4c61-8522-a87e48e18679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52ba114f-79b7-48ab-9036-35086a009c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to fix this part\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# go through test dataset, make predictions and store them\n",
    "for idx, sample in enumerate(test_ds):\n",
    "    with torch.no_grad():\n",
    "        input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "        attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        predicted_probs = torch.softmax(outputs.logits, dim=-1) # might not need this part\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "        predictions.append(predicted_labels)\n",
    "        true_labels.append(sample['labels'].item())\n",
    "        break\n",
    "\n",
    "# concatenate predictions and convert true_labels to numpy array\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# save evaluation results to CSV\n",
    "results_df = pd.DataFrame({'true_labels': true_labels, 'predicted_labels': predictions})\n",
    "results_df.to_csv(f'{project_dir}/model_output/results_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "33bc125e-60ba-4a6f-ba88-f2bde1529d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0941,  0.0359,  0.0814,  ..., -0.0062,  0.3980,  0.5112],\n",
       "         [ 0.1513, -0.0367, -0.0594,  ...,  0.2902,  0.0253, -0.0724],\n",
       "         [-0.2850,  0.0045,  0.0165,  ...,  0.0619,  0.1465,  0.2341],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b552a1ae-d86a-43ad-9015-2d9ce4b2da46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutputs\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7684dbaa-8813-4d00-aeb8-93e8217182e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1258, 0.8742]], device='cuda:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "552595e0-da00-4081-b9f0-44116aad0cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62902bce-c8e2-49f3-b922-e8dfa7933bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b66a6da-877c-4953-b829-8cd09622e65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a26dfe2f-fc40-4db4-93dc-ab4533f1cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0532,  0.0105,  0.1573,  ..., -0.0784,  0.0507,  0.0030],\n",
       "         [ 0.2120,  0.1111,  0.2713,  ...,  0.1405,  0.2491, -0.1953],\n",
       "         [-0.0927,  0.0092,  0.0337,  ...,  0.0112,  0.2658,  0.3896],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e26acd81-6680-4ef1-9d18-491634ee9a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(outputs[1].cpu(), axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9744d368-968d-4f80-8ed1-70be2f0dd7c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4016e-02,  7.7888e-02,  9.6039e-02, -8.2409e-02, -6.6462e-02,\n",
       "         -4.2732e-02, -9.4730e-02, -1.3979e-01,  1.1531e-02,  2.0624e-02,\n",
       "         -8.4614e-02,  2.6945e-02, -6.0919e-02, -2.9137e-02, -1.4624e-03,\n",
       "         -2.7918e-02, -4.0000e-02, -1.4787e-02,  2.7168e-04, -3.8372e-02,\n",
       "          5.8050e-02,  9.3877e-02, -1.9294e-02,  4.8864e-02,  6.5255e-02,\n",
       "          1.5263e-02,  1.6097e-01,  2.2446e-02, -4.3947e-02, -3.5209e-02,\n",
       "          6.4121e-03,  4.8512e-02, -5.1276e-02, -1.2220e-02, -1.4189e-02,\n",
       "          3.0952e-02, -4.0141e-02, -6.2739e-02, -8.8032e-03, -2.0333e-02,\n",
       "         -1.2996e-01,  4.5327e-02,  4.5376e-02, -2.2441e-02, -4.5005e-02,\n",
       "          1.1020e-01, -7.9964e-02, -5.1466e-02, -2.2288e-03,  5.5574e-02,\n",
       "          1.1368e-03, -6.9144e-02, -1.8800e-02,  1.2017e-02, -3.2260e-02,\n",
       "          1.0311e-01,  1.0994e-01, -9.5220e-02, -9.4616e-02, -2.5957e-03,\n",
       "         -5.3884e-02,  5.6647e-02, -4.2787e-02, -7.5616e-02,  1.0208e-01,\n",
       "          4.5428e-02, -1.0113e-01, -1.7945e-02, -1.7338e-02,  5.1559e-02,\n",
       "          2.8455e-02,  4.0849e-02, -2.7782e-02, -3.5524e-02,  9.7706e-02,\n",
       "         -8.0516e-02,  8.4395e-03, -2.5830e-02,  5.6869e-02, -4.0625e-02,\n",
       "         -7.8456e-02,  2.8076e-02,  7.2907e-03, -4.7120e-02,  3.2361e-02,\n",
       "          5.2136e-02, -5.6711e-02, -1.6922e-02,  9.2786e-02,  1.8854e-02,\n",
       "         -6.1260e-02,  4.1368e-02, -7.9893e-02, -3.6794e-02, -4.3266e-02,\n",
       "          8.1423e-02,  1.6940e-03, -2.6372e-02,  3.3756e-02,  2.0545e-03,\n",
       "          8.2581e-03,  4.9830e-03, -7.1282e-02,  5.7621e-02,  1.2289e-02,\n",
       "          4.8968e-02, -1.0340e-01,  3.3646e-02,  4.6704e-02,  5.2460e-02,\n",
       "         -9.5774e-03, -1.0496e-01, -4.4386e-02,  4.4653e-02, -1.1086e-01,\n",
       "          1.7012e-02,  8.7697e-02, -2.6715e-05, -1.8066e-02,  1.0940e-02,\n",
       "         -4.9956e-02,  5.0513e-02, -6.6977e-02, -6.8030e-02, -1.8158e-02,\n",
       "         -1.8731e-01,  1.1664e-01,  1.0746e-01,  8.3479e-02,  5.6482e-02,\n",
       "          5.1211e-02,  6.1631e-02, -1.2711e-02, -3.3624e-02,  2.6253e-02,\n",
       "          1.0698e-03,  2.2849e-02, -1.1370e-02, -4.1806e-02,  3.9603e-02,\n",
       "         -1.3899e-03, -1.0540e-01, -6.2941e-02,  7.6185e-02,  2.2045e-02,\n",
       "         -1.4801e-02, -1.0323e-01,  9.4301e-02,  1.5707e-02,  8.4795e-02,\n",
       "         -9.8185e-03, -2.9524e-02,  1.1172e-02,  3.6635e-02,  7.0130e-02,\n",
       "          2.1987e-02,  8.2221e-02,  6.1612e-02, -1.0862e-01,  8.9529e-02,\n",
       "          1.2182e-02,  8.0514e-03,  1.5483e-02, -1.7428e-02,  5.7345e-02,\n",
       "         -7.7966e-02,  4.6339e-02,  3.8210e-02, -1.0891e-01,  1.2828e-02,\n",
       "         -1.1366e-01,  3.7007e-02, -7.9215e-02,  1.2172e-01, -4.0606e-02,\n",
       "          8.8859e-02, -4.8300e-02, -1.2288e-01, -4.4201e-02, -2.7343e-03,\n",
       "          4.9484e-02, -4.3551e-02,  9.8814e-02, -9.3913e-02,  1.2761e-01,\n",
       "          3.9079e-03,  7.8941e-03, -2.9471e-02,  8.5199e-03,  3.0191e-02,\n",
       "          9.9916e-02,  6.1036e-02,  2.5724e-02,  4.1261e-02, -6.5281e-02,\n",
       "         -1.1624e-01, -2.9300e-02,  1.9238e-03,  5.1423e-02, -1.0204e-02,\n",
       "          1.0807e-01, -4.6332e-02,  5.9446e-02,  9.6639e-02,  1.1969e-02,\n",
       "         -7.7331e-02, -1.7034e-02,  9.6699e-02, -8.4453e-02, -1.0041e-01,\n",
       "          6.7950e-02,  9.1504e-02, -1.6416e-01,  1.8498e-02,  2.3892e-03,\n",
       "          9.0203e-02,  2.4822e-02, -7.9005e-02, -4.0367e-02,  3.0905e-03,\n",
       "          1.0925e-02,  2.2648e-02, -5.3190e-03,  3.9780e-02, -2.3407e-02,\n",
       "          8.5461e-04, -4.5960e-02,  7.9148e-02,  3.7385e-02, -9.1273e-03,\n",
       "          6.1764e-02, -1.1899e-01, -5.4494e-02,  8.9801e-02,  2.6286e-02,\n",
       "         -2.4073e-03, -8.8097e-02,  1.2135e-01,  1.5876e-02, -4.4351e-02,\n",
       "          4.9663e-02, -8.5629e-02,  4.0630e-02,  1.2418e-02,  1.4355e-02,\n",
       "          3.5522e-02, -1.9585e-02,  1.5152e-02,  6.7892e-02, -4.0686e-02,\n",
       "         -7.5658e-02, -2.4097e-03, -3.1616e-02,  7.5595e-02,  3.0473e-02,\n",
       "         -3.5104e-04,  2.0138e-02, -3.7472e-02, -5.7978e-03, -5.9250e-02,\n",
       "          1.8543e-02, -1.0464e-01, -4.7254e-02,  1.0456e-01,  3.1128e-03,\n",
       "          2.7271e-03, -5.7371e-02, -6.9266e-02, -2.3495e-03,  7.2197e-02,\n",
       "         -2.2899e-02,  5.1549e-02,  7.6459e-02, -1.9670e-02, -3.7559e-02,\n",
       "          1.1211e-01, -3.5047e-02,  1.0901e-02, -3.0021e-03,  2.4698e-02,\n",
       "          1.1215e-01, -8.8445e-02, -5.6903e-02,  1.6393e-01,  5.5335e-02,\n",
       "         -1.4294e-01,  2.9316e-02,  6.1771e-02,  5.7398e-02, -8.5095e-03,\n",
       "         -1.4673e-02,  9.1583e-03, -2.7573e-03,  5.2629e-03,  1.0114e-01,\n",
       "         -4.0461e-02,  5.7989e-02,  8.2613e-02,  4.3646e-02,  7.6979e-02,\n",
       "         -7.7576e-02, -1.7671e-02,  5.7784e-02,  9.5570e-03,  1.6285e-01,\n",
       "         -3.3305e-02,  3.6964e-02, -2.4526e-03, -2.7527e-02, -4.1108e-02,\n",
       "         -2.9858e-03, -7.8174e-02, -1.5577e-02, -7.4276e-02, -3.1343e-02,\n",
       "          3.3412e-02,  1.3664e-01,  8.4057e-02,  1.0259e-01,  1.7836e-02,\n",
       "          2.7761e-02,  1.4399e-01, -2.1732e-02, -6.2141e-02,  1.8582e-02,\n",
       "          5.4332e-03,  1.3266e-01,  6.9143e-02,  8.9055e-02, -1.7316e-01,\n",
       "          4.8561e-02,  9.2377e-02, -8.7179e-02, -5.9628e-02, -7.4734e-02,\n",
       "          8.6178e-02,  4.6214e-02, -1.2331e-01, -2.8799e-02, -3.0775e-02,\n",
       "         -4.3122e-02, -1.0569e-01,  2.6129e-02,  5.7825e-02,  3.3923e-02,\n",
       "          5.6526e-02, -1.0591e-01, -1.1240e-01, -5.9189e-02, -6.0449e-02,\n",
       "          4.4020e-02, -8.5117e-02,  5.8164e-03,  6.2278e-03,  4.6328e-02,\n",
       "          8.1918e-02,  2.9131e-02, -6.3537e-01,  3.3778e-02,  4.2087e-02,\n",
       "          2.3300e-02,  1.0085e-02, -9.0817e-02, -1.3846e-01,  3.9014e-02,\n",
       "          5.5854e-02, -1.5865e-01, -1.9242e-02,  3.5988e-02,  7.3556e-02,\n",
       "          3.4734e-02,  6.7393e-02, -4.3876e-02, -5.2610e-02,  4.4584e-02,\n",
       "          6.2366e-02, -8.0804e-02, -9.4248e-03, -5.1238e-02,  1.4673e-02,\n",
       "          6.5958e-02,  2.4785e-02,  6.6066e-02, -2.5715e-02,  6.5472e-02,\n",
       "          1.3840e-01, -1.3868e-02,  9.0004e-02, -2.6449e-02,  8.2847e-02,\n",
       "          5.2571e-02,  1.3702e-01, -5.5554e-02,  8.0751e-02, -6.5848e-02,\n",
       "         -6.1566e-02, -1.0503e-01,  1.3736e-01,  4.3268e-02,  2.0804e-02,\n",
       "         -9.4998e-02,  1.9981e-02, -1.3219e-02,  8.8078e-02,  1.4113e-01,\n",
       "         -8.7223e-02,  1.2085e-01,  1.0326e-01,  5.9230e-02,  2.9697e-02,\n",
       "          1.1576e-01,  2.1448e-02, -2.0428e-02,  5.5280e-02, -4.9404e-02,\n",
       "         -2.0326e-02, -9.4042e-02,  8.0785e-02,  1.6690e-01, -1.3879e-01,\n",
       "          5.5813e-02,  1.6954e-02, -6.1053e-02, -9.4175e-02, -1.0613e-02,\n",
       "          7.4430e-02,  1.2059e-01, -7.3422e-02,  1.1435e-02,  7.9830e-03,\n",
       "         -8.4588e-02, -2.9817e-03,  5.5944e-02,  8.8732e-02, -1.2260e-02,\n",
       "         -7.0406e-02,  1.2532e-02,  2.4941e-02, -3.5905e-02, -5.6854e-01,\n",
       "          2.1742e-02,  9.6106e-02,  3.3838e-02, -4.4231e-02,  3.9232e-02,\n",
       "          3.3600e-02,  2.9823e-02,  1.4611e-02, -7.9156e-02, -8.4573e-02,\n",
       "          1.0077e-01,  1.4399e-02,  4.0600e-02, -4.2753e-02, -1.3199e-01,\n",
       "          1.7712e-02,  1.2746e-02, -3.6991e-02,  2.8362e-02, -1.3698e-01,\n",
       "          6.1151e-02, -3.6180e-02, -6.6730e-02,  1.7602e-02, -1.6167e-02,\n",
       "          3.6193e-02, -8.7656e-02, -7.0721e-02,  5.6459e-02,  6.0021e-02,\n",
       "         -3.8461e-02, -1.4433e-02,  6.8979e-03,  1.3712e-01, -8.5741e-03,\n",
       "          1.5400e-02, -3.3580e-02,  4.5379e-02,  3.9395e-02,  3.3389e-02,\n",
       "         -9.3301e-02, -8.7253e-02, -3.9559e-02,  5.9870e-02, -2.1492e-02,\n",
       "         -2.9692e-02,  1.7161e-02,  6.9611e-02,  1.5119e-01,  6.5290e-02,\n",
       "          1.0044e-01,  8.0685e-02,  4.2180e-02, -4.4343e-02,  9.0362e-02,\n",
       "         -9.1631e-03,  5.2377e-02,  7.7197e-02,  7.8536e-02,  2.1301e-02,\n",
       "          1.2732e-01,  7.4599e-02, -1.6400e-02, -2.3809e-02,  8.3664e-02,\n",
       "          4.4486e-02,  1.5913e-02, -7.4231e-02, -9.4353e-02,  3.5670e-02,\n",
       "          8.8035e-02, -3.5487e-02,  2.2616e-04, -2.5848e-02,  7.6393e-02,\n",
       "         -1.3922e-01, -4.8037e-02,  5.1570e-02,  4.1651e-02, -2.3640e-02,\n",
       "          3.3907e-02,  5.2922e-02,  1.9218e-02,  9.9704e-02, -3.3624e-02,\n",
       "         -1.4684e-01,  8.6141e-02,  2.3344e-02,  5.4360e-03,  5.1902e-02,\n",
       "         -3.9470e-02,  2.8058e-02,  5.2653e-02,  9.3914e-02,  2.1011e-02,\n",
       "         -6.0069e-02, -6.8979e-02, -4.9001e-02, -1.4672e-02,  6.3183e-02,\n",
       "         -2.1511e-03, -8.1776e-02, -4.0620e-02,  2.0347e-03,  2.5331e-02,\n",
       "         -6.0273e-02,  4.7230e-02,  8.4947e-02, -7.1120e-02, -3.3792e-02,\n",
       "         -2.9591e-01,  2.4048e-02,  4.6619e-02,  2.5128e-02, -1.7859e-02,\n",
       "          6.0582e-02,  9.0231e-02,  3.4516e-02,  1.5959e-02,  3.7031e-02,\n",
       "         -7.5549e-02,  2.5297e-02,  5.9966e-02, -8.9942e-03,  2.8834e-02,\n",
       "          1.4478e-01,  1.4511e-03,  5.2149e-02,  2.8962e-02,  1.2064e-01,\n",
       "          1.1443e-01,  3.9209e-02, -3.5150e-02, -8.8571e-02, -8.9495e-02,\n",
       "         -5.5623e-02,  3.0770e-04, -3.8260e-02,  7.4052e-02, -3.6402e-02,\n",
       "         -4.8802e-02, -7.2122e-02,  7.5547e-02,  4.5937e-04,  5.3474e-02,\n",
       "          1.3678e-02, -5.4765e-02,  1.7388e-02, -1.6043e-02, -2.3671e-02,\n",
       "          5.2866e-02, -1.2908e-01,  3.0213e-02,  7.3346e-02,  6.6759e-02,\n",
       "         -5.3616e-02,  1.2010e-01,  1.3594e-02,  4.0687e-04,  3.2383e-02,\n",
       "         -1.4651e-02,  4.1015e-02,  9.7285e-02, -9.6336e-02,  2.4638e-03,\n",
       "         -5.9688e-02,  9.8833e-02, -1.0116e-01,  6.5142e-03, -8.1073e-03,\n",
       "          4.9756e-02, -6.0684e-03, -5.0033e-02,  3.1661e-02,  6.6815e-02,\n",
       "         -3.5241e-02, -6.7743e-03, -5.2418e-02,  7.2884e-02,  1.3167e-02,\n",
       "         -4.7677e-02,  3.4364e-02, -2.1727e-02, -1.0167e-02,  5.4499e-02,\n",
       "         -5.8089e-02, -7.7070e-02,  7.5389e-02, -3.4605e-02, -6.5962e-03,\n",
       "          4.5748e-02,  2.0352e-02,  8.6682e-03,  8.3176e-02,  1.0642e-01,\n",
       "          5.0201e-02, -7.4567e-02, -1.0892e-02, -7.7197e-02, -2.0693e-02,\n",
       "         -7.5510e-02, -1.8964e-01, -2.7168e-02, -4.0594e-02,  5.4090e-02,\n",
       "          1.4310e-02,  8.0311e-02, -4.3233e-02, -1.1572e-01, -5.6956e-03,\n",
       "         -5.1734e-02,  6.8607e-03, -4.0356e-02, -9.4166e-03,  8.2736e-03,\n",
       "         -1.3030e-02, -3.1064e-03,  3.3577e-02, -6.3110e-02, -2.6653e-02,\n",
       "         -4.2481e-02, -1.1640e-01, -2.5889e-02,  1.2518e-01,  7.3702e-02,\n",
       "         -1.6200e-02,  5.5381e-02,  7.9414e-02,  3.9182e-02,  1.3100e-02,\n",
       "         -4.8291e-02, -7.4341e-02,  1.4690e-02,  2.6345e-02,  5.6189e-03,\n",
       "          7.3642e-02,  4.1354e-04,  4.6157e-02,  9.5591e-03,  8.2454e-02,\n",
       "          8.7878e-02, -1.1810e-01, -1.3952e-02, -9.2462e-02,  6.4359e-02,\n",
       "          5.3780e-02,  6.6510e-02, -9.3239e-02, -6.4234e-03, -3.9505e-03,\n",
       "          4.0594e-02, -1.5305e-02, -1.5303e-02, -4.3594e-02, -6.8061e-02,\n",
       "         -4.2975e-02,  1.3818e-03,  3.2886e-03, -2.8575e-02, -6.5097e-02,\n",
       "          3.3157e-03, -5.7245e-02,  7.1031e-02,  5.5831e-02,  2.6078e-02,\n",
       "         -5.1930e-02,  5.0515e-02,  8.2311e-02, -2.7687e-02, -8.4165e-02,\n",
       "         -4.8007e-02,  1.4697e-01,  9.4378e-02,  5.8739e-02, -5.4458e-02,\n",
       "         -5.6772e-02,  6.5648e-02, -7.1324e-03, -1.0210e-01,  9.7032e-02,\n",
       "          9.8381e-02,  5.0617e-02, -6.7828e-02, -4.0558e-02,  4.6595e-02,\n",
       "          8.8193e-02, -1.8287e-01, -8.0017e-03, -9.2992e-02, -7.1240e-02,\n",
       "          9.6584e-02, -4.3428e-02, -6.9049e-02,  1.1651e-01, -1.6537e-02,\n",
       "          8.9422e-03,  2.5550e-02, -5.2208e-02, -4.9158e-02,  1.7340e-02,\n",
       "          6.2837e-02, -6.4638e-02,  4.4159e-03,  1.7323e-02, -4.6451e-02,\n",
       "          5.9525e-02,  1.6065e-02,  8.1355e-02, -9.6899e-02,  3.5351e-02,\n",
       "          2.4043e-02, -8.0031e-02,  1.2527e-02,  9.2171e-02,  3.0833e-02,\n",
       "          7.5700e-02, -1.2019e-02,  1.4000e-02,  1.8523e-02, -1.0238e-02,\n",
       "         -3.6221e-02, -2.5854e-02, -2.4680e-04, -2.6455e-02,  7.7115e-02,\n",
       "          2.3861e-02,  3.7743e-02,  1.2917e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(outputs[1], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42631321-4def-44d5-980e-5eb01f455135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0df41ad3-e43d-44d3-adc0-ee92f2c93e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9755e22d-b6b7-4391-a461-49c151412083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988f1ba-8917-4afd-bc36-f7f698d01c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189efbae-3489-44a1-81a7-01ed8cc926fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247a456-2a55-4fc2-bc1b-81a2187a20fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d566c85b-cb2d-4f5a-b7bd-a9ce83b1adc0",
   "metadata": {},
   "source": [
    "### test sequence for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1dd540d4-ec6e-4499-9aa8-e26d2ca50e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    AutoModelForSequenceClassification, \n",
    "    DataCollatorForTokenClassification, \n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from transformers.models.bert.configuration_bert import BertConfig \n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\")\n",
    "#model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, config=config)\n",
    "#model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, padding=True)\n",
    "\n",
    "sequence = 'CTCTTTTTTGATGCGAATACCTGTTGTGGCGGAAACTGAAGTCCCTGCCCATCAGCTGTTCGACGAGTTTGCGTTGTTCTGCGGCAGGCGTTGACATAGTTGACATTCTGACGGTAGTTGGCAGCTTTCTGCTGGGAGTGTAGAGCCTTTTGTAGTATAACTTTTTGTTCTTTTCTTTCTTCTCTAATGCCTGAGGCTTCGGCGGTTATTCGGGTAATACATTAAGGAAGTTGCCATGATGTGGAAGAATACGACTAGTCAGTTAGCGATGGCCAGCTCCTTTACCTAAATCATGTGGCCTATCTTCAGATAGCATACTACCACCAACCATCAATACCTCAATGGCTTTCAACAAGTACCCTTCGTCGGGTTTTCAGCTTTTTTCCTACTTTGCTGGATGCTTTGTGAAGATCAGGAAGAAATTTTATCGGAAAAACCTCACTAGGAGAAACTATATAAAAGAAGGGTCAGACTGAAAATGACGCCGTAATAAACTCGTG'\n",
    "label = 1\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9814909f-a559-4f1e-a74a-d8049cf39e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9649deec-4250-427c-9571-c941ff2160ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9ab8805b-bd8f-47c8-a2a8-6ce8b0d053b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9527,  0.9219]], device='cuda:0'), hidden_states=tensor([[[-0.1754, -0.0160,  0.1625,  ...,  0.0359,  0.2485,  0.2095],\n",
       "         [ 0.2011, -0.0184, -0.0584,  ...,  0.2842,  0.0031, -0.1131],\n",
       "         [-0.2527, -0.0199,  0.0328,  ...,  0.0667,  0.1246,  0.1658],\n",
       "         ...,\n",
       "         [-0.0112, -0.1188,  0.1362,  ...,  0.2800,  0.0241, -0.1264],\n",
       "         [ 0.2483,  0.5387,  0.0147,  ..., -0.1258,  0.1126,  0.1182],\n",
       "         [-0.1736,  0.0155,  0.0341,  ..., -0.0446,  0.1998,  0.3108]]],\n",
       "       device='cuda:0'), attentions=None)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c08479-ca6b-41be-88cc-b2b7eff321bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna [~/.conda/envs/dna/]",
   "language": "python",
   "name": "conda_dna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
