{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60a65e5-4d03-41a4-9fe3-4731012cad99",
   "metadata": {},
   "source": [
    "# Fine-tuning script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d251927-6ec1-4a38-ae01-5cfb2b45fb55",
   "metadata": {},
   "source": [
    "*Anusha, Joyce, Nina*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459bf7cb-8cb3-49b8-9d1c-98ff46686ffd",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6f60bbc-a2e8-4ad3-acdf-0ceba0a38fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    AutoModelForSequenceClassification, \n",
    "    DataCollatorForTokenClassification, \n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from transformers.models.bert.configuration_bert import BertConfig \n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "#import Dataset\n",
    "import os\n",
    "\n",
    "import csv\n",
    "from Bio import SeqIO\n",
    "\n",
    "#os.environ['WANDB_NOTEBOOK_NAME'] = 'fine-tune'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "89d38251-2066-40f6-84f3-c4412cba2e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ids and comment out others when using\n",
    "#princeton_id = 'aa8417'\n",
    "princeton_id = 'ns5404'\n",
    "#princeton_id = 'jf...'\n",
    "\n",
    "project_dir = f'/scratch/gpfs/{princeton_id}/QCB557_project'\n",
    "\n",
    "model_name = 'fine_tune_v2'\n",
    "model_out_dir = f'{project_dir}/models/{model_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89162e24-9c9d-4777-8310-3927638dacce",
   "metadata": {},
   "source": [
    "#### Load model and tokenizer from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f9826f6a-6d87-47ee-8778-d9700397329a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# use gpu\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\")\n",
    "print(config.num_labels) #2 labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, config=config)\n",
    "\n",
    "# don't need to move the tokenizer to gpu b/c it's light\n",
    "# use data collator to pad sequences dynamically during training\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, padding=True)\n",
    "tokenizer.pad_token = \"X\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804470ee-28a9-4a7b-a8a5-e72651dfcb52",
   "metadata": {
    "tags": []
   },
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f4262-11eb-4aae-81f2-62c264d798ea",
   "metadata": {},
   "source": [
    "#### Freeze and unfreeze gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a742af6-b655-4584-8605-62d74da0416f",
   "metadata": {},
   "source": [
    "We can play around with this later as we imrove the performance of the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0f61d91-4d77-479c-b3e3-b622d5808ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default when using AutoModelForSequenceClassification is that all pretrained parameters/layers are frozen except classification head\n",
    "# as we go through rounds of fine-tuning, we can optionally unfreeze from of the pretrained layers to improve performance\n",
    "\n",
    "# unfreeze the last layer in the encoder block\n",
    "for name, param in model.named_parameters():\n",
    "    if \"encoder.layer.11\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9330e2f3-e3ab-4772-a02c-1ee2100b73ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f4bca-6dc8-4898-a2b7-9930fc378101",
   "metadata": {},
   "source": [
    "#### Load data from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b9a8ad43-cd08-45f6-8ff8-e6698343d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_data_load(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, shuffle=True):\n",
    "        if shuffle:\n",
    "            self.dataframe = dataframe.sample(frac=1).reset_index(drop=True)  # shuffle the dataframe\n",
    "        else:\n",
    "            self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.dataframe.iloc[idx]['sequence']\n",
    "        #print(sequence)\n",
    "        #print(len(sequence))\n",
    "        label = (self.dataframe.iloc[idx]['label'])\n",
    "        #print(label)\n",
    "\n",
    "        # tokenize the sequence\n",
    "        # tokenizer automatically generates attention masks\n",
    "        #inputs = self.tokenizer(sequence, padding='max_length', max_length=500, truncation=True, return_tensors='pt')\n",
    "        inputs = self.tokenizer(sequence, padding='max_length', max_length=128, return_tensors='pt')\n",
    "        #print(inputs)\n",
    "        \n",
    "        # move inputs to gpu\n",
    "        #inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': (torch.tensor([label], dtype=torch.long))\n",
    "            #'labels': torch.tensor([int(label)]) \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da922b8-534d-4773-a9fe-ac32e9b9eb32",
   "metadata": {},
   "source": [
    "#### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc6ed7f8-af00-4d7d-b63f-3c9f7fcf2a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_to_split = pd.read_csv(f'{project_dir}/data/train.csv')\n",
    "train_data_to_split['label'] = train_data_to_split['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "83c0ca0d-f531-435e-bfcc-a46d801cd68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(f'{project_dir}/data/test.csv')\n",
    "test_data['label'] = test_data['label'].astype(int)\n",
    "\n",
    "test_ds = custom_data_load(dataframe = test_data, tokenizer = tokenizer, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfda9b5-0eaa-402c-986f-f816eb07faa5",
   "metadata": {},
   "source": [
    "#### Split training into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "37dc3169-165e-47f9-a278-ab2de4ac3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_test_split(train_data_to_split, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f0d4b7e-2a6b-4b07-b7be-43e3eedc27da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = custom_data_load(dataframe = train_data, tokenizer = tokenizer, shuffle=True)\n",
    "valid_ds = custom_data_load(dataframe = valid_data, tokenizer = tokenizer, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e9632a30-e60f-4079-87df-e05b8902baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataLoader(train_ds, pin_memory=True)\n",
    "valid = DataLoader(valid_ds, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec92215c-4c80-4c3d-8421-f92cd5702014",
   "metadata": {},
   "source": [
    "#### Data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7485901b-5614-4d4b-a668-74e6faf88aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer, label_pad_token_id='X')\n",
    "#data_collator = DataCollatorWithPadding(tokenizer)\n",
    "#data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da355e56-064b-4625-ba54-1aeaa748be55",
   "metadata": {},
   "source": [
    "#### Training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b8aa17d-a48b-47bb-a685-413713e56bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = model_out_dir,\n",
    "    num_train_epochs= 10, \n",
    "    per_device_train_batch_size=8, # powers of 2\n",
    "    weight_decay=0.015, # regularization\n",
    "    learning_rate=1e-5,\n",
    "    gradient_accumulation_steps=2, # how many batches of gradients are accumulated before parameter update\n",
    "    #gradient_checkpointing=True, # helps reduce memory\n",
    "    #dataloader_num_workers=4,\n",
    "    \n",
    "    log_level=\"error\",\n",
    "    evaluation_strategy=\"steps\",  \n",
    "    eval_steps=500,        \n",
    "    logging_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    save_strategy=\"no\", # don't want to save checkpoints -- takes up too much space, can change later\n",
    "    fp16=True,\n",
    "    #dataloader_pin_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242bb20-c06f-4a4a-8bcd-d10a9f05194a",
   "metadata": {},
   "source": [
    "#### Extra metrics to compute with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4d3fd445-39f5-48e9-9834-a7d32e194b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to fix this, doesn't work with current prediction + label format\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions)\n",
    "    recall = recall_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions)\n",
    "    \n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd838666-2126-4080-a63c-849c2d97af5b",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "283f8d8e-dea0-4a9e-b72c-c1aff50508c0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6369, 'learning_rate': 9.73161567364466e-06, 'epoch': 0.27}\n",
      "{'eval_loss': 0.666924238204956, 'eval_runtime': 5.7938, 'eval_samples_per_second': 571.641, 'eval_steps_per_second': 71.455, 'epoch': 0.27}\n",
      "{'loss': 0.6426, 'learning_rate': 9.463231347289318e-06, 'epoch': 0.54}\n",
      "{'eval_loss': 0.657680332660675, 'eval_runtime': 5.8656, 'eval_samples_per_second': 564.648, 'eval_steps_per_second': 70.581, 'epoch': 0.54}\n",
      "{'loss': 0.6434, 'learning_rate': 9.194847020933978e-06, 'epoch': 0.81}\n",
      "{'eval_loss': 0.6539648175239563, 'eval_runtime': 5.8119, 'eval_samples_per_second': 569.862, 'eval_steps_per_second': 71.233, 'epoch': 0.81}\n",
      "{'loss': 0.6372, 'learning_rate': 8.926462694578637e-06, 'epoch': 1.07}\n",
      "{'eval_loss': 0.659578263759613, 'eval_runtime': 5.9892, 'eval_samples_per_second': 552.996, 'eval_steps_per_second': 69.125, 'epoch': 1.07}\n",
      "{'loss': 0.6278, 'learning_rate': 8.658615136876008e-06, 'epoch': 1.34}\n",
      "{'eval_loss': 0.6537167429924011, 'eval_runtime': 6.0085, 'eval_samples_per_second': 551.222, 'eval_steps_per_second': 68.903, 'epoch': 1.34}\n",
      "{'loss': 0.6265, 'learning_rate': 8.390230810520667e-06, 'epoch': 1.61}\n",
      "{'eval_loss': 0.6545549035072327, 'eval_runtime': 6.0141, 'eval_samples_per_second': 550.707, 'eval_steps_per_second': 68.838, 'epoch': 1.61}\n",
      "{'loss': 0.6275, 'learning_rate': 8.121846484165325e-06, 'epoch': 1.88}\n",
      "{'eval_loss': 0.650616466999054, 'eval_runtime': 6.2143, 'eval_samples_per_second': 532.965, 'eval_steps_per_second': 66.621, 'epoch': 1.88}\n",
      "{'loss': 0.619, 'learning_rate': 7.853462157809985e-06, 'epoch': 2.15}\n",
      "{'eval_loss': 0.651303768157959, 'eval_runtime': 6.1617, 'eval_samples_per_second': 537.516, 'eval_steps_per_second': 67.19, 'epoch': 2.15}\n",
      "{'loss': 0.623, 'learning_rate': 7.585614600107354e-06, 'epoch': 2.42}\n",
      "{'eval_loss': 0.6471772193908691, 'eval_runtime': 6.0295, 'eval_samples_per_second': 549.296, 'eval_steps_per_second': 68.662, 'epoch': 2.42}\n",
      "{'loss': 0.6152, 'learning_rate': 7.317230273752014e-06, 'epoch': 2.68}\n",
      "{'eval_loss': 0.65521639585495, 'eval_runtime': 6.1913, 'eval_samples_per_second': 534.94, 'eval_steps_per_second': 66.868, 'epoch': 2.68}\n",
      "{'loss': 0.6195, 'learning_rate': 7.048845947396673e-06, 'epoch': 2.95}\n",
      "{'eval_loss': 0.6447626948356628, 'eval_runtime': 6.1324, 'eval_samples_per_second': 540.083, 'eval_steps_per_second': 67.51, 'epoch': 2.95}\n",
      "{'loss': 0.6198, 'learning_rate': 6.780461621041332e-06, 'epoch': 3.22}\n",
      "{'eval_loss': 0.6452792882919312, 'eval_runtime': 6.0713, 'eval_samples_per_second': 545.517, 'eval_steps_per_second': 68.19, 'epoch': 3.22}\n",
      "{'loss': 0.6109, 'learning_rate': 6.512614063338701e-06, 'epoch': 3.49}\n",
      "{'eval_loss': 0.6469985842704773, 'eval_runtime': 6.2097, 'eval_samples_per_second': 533.361, 'eval_steps_per_second': 66.67, 'epoch': 3.49}\n",
      "{'loss': 0.6104, 'learning_rate': 6.24422973698336e-06, 'epoch': 3.76}\n",
      "{'eval_loss': 0.6494410634040833, 'eval_runtime': 6.1927, 'eval_samples_per_second': 534.822, 'eval_steps_per_second': 66.853, 'epoch': 3.76}\n",
      "{'loss': 0.6069, 'learning_rate': 5.97584541062802e-06, 'epoch': 4.03}\n",
      "{'eval_loss': 0.6497074961662292, 'eval_runtime': 6.2325, 'eval_samples_per_second': 531.41, 'eval_steps_per_second': 66.426, 'epoch': 4.03}\n",
      "{'loss': 0.6086, 'learning_rate': 5.707461084272678e-06, 'epoch': 4.29}\n",
      "{'eval_loss': 0.6440463662147522, 'eval_runtime': 6.1758, 'eval_samples_per_second': 536.284, 'eval_steps_per_second': 67.035, 'epoch': 4.29}\n",
      "{'loss': 0.6067, 'learning_rate': 5.439613526570049e-06, 'epoch': 4.56}\n",
      "{'eval_loss': 0.6454474925994873, 'eval_runtime': 6.0273, 'eval_samples_per_second': 549.501, 'eval_steps_per_second': 68.688, 'epoch': 4.56}\n",
      "{'loss': 0.606, 'learning_rate': 5.171229200214708e-06, 'epoch': 4.83}\n",
      "{'eval_loss': 0.6515025496482849, 'eval_runtime': 6.2116, 'eval_samples_per_second': 533.196, 'eval_steps_per_second': 66.649, 'epoch': 4.83}\n",
      "{'loss': 0.5959, 'learning_rate': 4.902844873859367e-06, 'epoch': 5.1}\n",
      "{'eval_loss': 0.6512770056724548, 'eval_runtime': 6.2874, 'eval_samples_per_second': 526.766, 'eval_steps_per_second': 65.846, 'epoch': 5.1}\n",
      "{'loss': 0.601, 'learning_rate': 4.634460547504026e-06, 'epoch': 5.37}\n",
      "{'eval_loss': 0.6489976048469543, 'eval_runtime': 5.9584, 'eval_samples_per_second': 555.857, 'eval_steps_per_second': 69.482, 'epoch': 5.37}\n",
      "{'loss': 0.6043, 'learning_rate': 4.366612989801396e-06, 'epoch': 5.64}\n",
      "{'eval_loss': 0.64485764503479, 'eval_runtime': 6.0491, 'eval_samples_per_second': 547.519, 'eval_steps_per_second': 68.44, 'epoch': 5.64}\n",
      "{'loss': 0.5969, 'learning_rate': 4.098228663446055e-06, 'epoch': 5.9}\n",
      "{'eval_loss': 0.6467708349227905, 'eval_runtime': 6.2072, 'eval_samples_per_second': 533.578, 'eval_steps_per_second': 66.697, 'epoch': 5.9}\n",
      "{'loss': 0.5943, 'learning_rate': 3.829844337090714e-06, 'epoch': 6.17}\n",
      "{'eval_loss': 0.6469480991363525, 'eval_runtime': 6.1641, 'eval_samples_per_second': 537.303, 'eval_steps_per_second': 67.163, 'epoch': 6.17}\n",
      "{'loss': 0.5943, 'learning_rate': 3.5614600107353738e-06, 'epoch': 6.44}\n",
      "{'eval_loss': 0.6548708081245422, 'eval_runtime': 5.7707, 'eval_samples_per_second': 573.938, 'eval_steps_per_second': 71.742, 'epoch': 6.44}\n",
      "{'loss': 0.5987, 'learning_rate': 3.2936124530327428e-06, 'epoch': 6.71}\n",
      "{'eval_loss': 0.6448161005973816, 'eval_runtime': 6.0775, 'eval_samples_per_second': 544.957, 'eval_steps_per_second': 68.12, 'epoch': 6.71}\n",
      "{'loss': 0.5994, 'learning_rate': 3.0252281266774023e-06, 'epoch': 6.98}\n",
      "{'eval_loss': 0.6442272663116455, 'eval_runtime': 5.7188, 'eval_samples_per_second': 579.145, 'eval_steps_per_second': 72.393, 'epoch': 6.98}\n",
      "{'loss': 0.5869, 'learning_rate': 2.7568438003220615e-06, 'epoch': 7.25}\n",
      "{'eval_loss': 0.6487972736358643, 'eval_runtime': 5.8064, 'eval_samples_per_second': 570.409, 'eval_steps_per_second': 71.301, 'epoch': 7.25}\n",
      "{'loss': 0.5959, 'learning_rate': 2.4884594739667206e-06, 'epoch': 7.51}\n",
      "{'eval_loss': 0.6439093351364136, 'eval_runtime': 5.8525, 'eval_samples_per_second': 565.913, 'eval_steps_per_second': 70.739, 'epoch': 7.51}\n",
      "{'loss': 0.5922, 'learning_rate': 2.2206119162640905e-06, 'epoch': 7.78}\n",
      "{'eval_loss': 0.644515335559845, 'eval_runtime': 5.799, 'eval_samples_per_second': 571.128, 'eval_steps_per_second': 71.391, 'epoch': 7.78}\n",
      "{'loss': 0.5883, 'learning_rate': 1.9522275899087496e-06, 'epoch': 8.05}\n",
      "{'eval_loss': 0.64539635181427, 'eval_runtime': 6.1657, 'eval_samples_per_second': 537.168, 'eval_steps_per_second': 67.146, 'epoch': 8.05}\n",
      "{'loss': 0.5897, 'learning_rate': 1.6838432635534086e-06, 'epoch': 8.32}\n",
      "{'eval_loss': 0.6452213525772095, 'eval_runtime': 5.8999, 'eval_samples_per_second': 561.37, 'eval_steps_per_second': 70.171, 'epoch': 8.32}\n",
      "{'loss': 0.5924, 'learning_rate': 1.415458937198068e-06, 'epoch': 8.59}\n",
      "{'eval_loss': 0.6457920670509338, 'eval_runtime': 6.1411, 'eval_samples_per_second': 539.313, 'eval_steps_per_second': 67.414, 'epoch': 8.59}\n",
      "{'loss': 0.5914, 'learning_rate': 1.1476113794954375e-06, 'epoch': 8.86}\n",
      "{'eval_loss': 0.6450211405754089, 'eval_runtime': 6.2645, 'eval_samples_per_second': 528.693, 'eval_steps_per_second': 66.087, 'epoch': 8.86}\n",
      "{'loss': 0.5945, 'learning_rate': 8.792270531400967e-07, 'epoch': 9.13}\n",
      "{'eval_loss': 0.6451196074485779, 'eval_runtime': 6.0463, 'eval_samples_per_second': 547.772, 'eval_steps_per_second': 68.472, 'epoch': 9.13}\n",
      "{'loss': 0.5846, 'learning_rate': 6.108427267847558e-07, 'epoch': 9.39}\n",
      "{'eval_loss': 0.6444507837295532, 'eval_runtime': 6.0102, 'eval_samples_per_second': 551.059, 'eval_steps_per_second': 68.882, 'epoch': 9.39}\n",
      "{'loss': 0.5887, 'learning_rate': 3.424584004294149e-07, 'epoch': 9.66}\n",
      "{'eval_loss': 0.6451740264892578, 'eval_runtime': 5.9769, 'eval_samples_per_second': 554.131, 'eval_steps_per_second': 69.266, 'epoch': 9.66}\n",
      "{'loss': 0.5887, 'learning_rate': 7.461084272678476e-08, 'epoch': 9.93}\n",
      "{'eval_loss': 0.645281970500946, 'eval_runtime': 5.9387, 'eval_samples_per_second': 557.702, 'eval_steps_per_second': 69.713, 'epoch': 9.93}\n",
      "{'train_runtime': 972.4329, 'train_samples_per_second': 306.52, 'train_steps_per_second': 19.158, 'train_loss': 0.6071107500929124, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    #compute_metrics=compute_metrics\n",
    "    #data_collator = data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "19742b20-14ba-462f-afc3-eca6246cd360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1ccecfa9-e5d8-4f76-a2bd-72bf6271cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.DataFrame(trainer.state.log_history)\n",
    "log_df.to_csv(f'{project_dir}/model_output/log_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e4aebea5-f5e3-4b32-8500-27d5ef294377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6369</td>\n",
       "      <td>9.731616e-06</td>\n",
       "      <td>0.27</td>\n",
       "      <td>500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.27</td>\n",
       "      <td>500</td>\n",
       "      <td>0.666924</td>\n",
       "      <td>5.7938</td>\n",
       "      <td>571.641</td>\n",
       "      <td>71.455</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.6426</td>\n",
       "      <td>9.463231e-06</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.657680</td>\n",
       "      <td>5.8656</td>\n",
       "      <td>564.648</td>\n",
       "      <td>70.581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6434</td>\n",
       "      <td>9.194847e-06</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.5887</td>\n",
       "      <td>3.424584e-07</td>\n",
       "      <td>9.66</td>\n",
       "      <td>18000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.66</td>\n",
       "      <td>18000</td>\n",
       "      <td>0.645174</td>\n",
       "      <td>5.9769</td>\n",
       "      <td>554.131</td>\n",
       "      <td>69.266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.5887</td>\n",
       "      <td>7.461084e-08</td>\n",
       "      <td>9.93</td>\n",
       "      <td>18500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.93</td>\n",
       "      <td>18500</td>\n",
       "      <td>0.645282</td>\n",
       "      <td>5.9387</td>\n",
       "      <td>557.702</td>\n",
       "      <td>69.713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.00</td>\n",
       "      <td>18630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>972.4329</td>\n",
       "      <td>306.52</td>\n",
       "      <td>19.158</td>\n",
       "      <td>2.607896e+16</td>\n",
       "      <td>0.607111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  learning_rate  epoch   step  eval_loss  eval_runtime  \\\n",
       "0   0.6369   9.731616e-06   0.27    500        NaN           NaN   \n",
       "1      NaN            NaN   0.27    500   0.666924        5.7938   \n",
       "2   0.6426   9.463231e-06   0.54   1000        NaN           NaN   \n",
       "3      NaN            NaN   0.54   1000   0.657680        5.8656   \n",
       "4   0.6434   9.194847e-06   0.81   1500        NaN           NaN   \n",
       "..     ...            ...    ...    ...        ...           ...   \n",
       "70  0.5887   3.424584e-07   9.66  18000        NaN           NaN   \n",
       "71     NaN            NaN   9.66  18000   0.645174        5.9769   \n",
       "72  0.5887   7.461084e-08   9.93  18500        NaN           NaN   \n",
       "73     NaN            NaN   9.93  18500   0.645282        5.9387   \n",
       "74     NaN            NaN  10.00  18630        NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "0                       NaN                    NaN            NaN   \n",
       "1                   571.641                 71.455            NaN   \n",
       "2                       NaN                    NaN            NaN   \n",
       "3                   564.648                 70.581            NaN   \n",
       "4                       NaN                    NaN            NaN   \n",
       "..                      ...                    ...            ...   \n",
       "70                      NaN                    NaN            NaN   \n",
       "71                  554.131                 69.266            NaN   \n",
       "72                      NaN                    NaN            NaN   \n",
       "73                  557.702                 69.713            NaN   \n",
       "74                      NaN                    NaN       972.4329   \n",
       "\n",
       "    train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n",
       "0                        NaN                     NaN           NaN         NaN  \n",
       "1                        NaN                     NaN           NaN         NaN  \n",
       "2                        NaN                     NaN           NaN         NaN  \n",
       "3                        NaN                     NaN           NaN         NaN  \n",
       "4                        NaN                     NaN           NaN         NaN  \n",
       "..                       ...                     ...           ...         ...  \n",
       "70                       NaN                     NaN           NaN         NaN  \n",
       "71                       NaN                     NaN           NaN         NaN  \n",
       "72                       NaN                     NaN           NaN         NaN  \n",
       "73                       NaN                     NaN           NaN         NaN  \n",
       "74                    306.52                  19.158  2.607896e+16    0.607111  \n",
       "\n",
       "[75 rows x 13 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb1cdc-fc1d-4248-b625-b96a3b2603b0",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ac0d94e4-711d-43d0-b53e-5c928b8753af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load fine-tuned model\n",
    "\n",
    "config = BertConfig.from_pretrained(f'{training_args.output_dir}/config.json')\n",
    "print(config.num_labels) #2 labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(training_args.output_dir, trust_remote_code=True, config=config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f4ed5d87-9785-4c61-8522-a87e48e18679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "52ba114f-79b7-48ab-9036-35086a009c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to fix this part\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# go through test dataset, make predictions and store them\n",
    "for idx, sample in enumerate(test_ds):\n",
    "    with torch.no_grad():\n",
    "        input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "        attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        #predicted_probs = torch.softmax(outputs.logits, dim=-1) # might not need this part\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "        predictions.append(predicted_labels)\n",
    "        true_labels.append(sample['labels'].item())\n",
    "\n",
    "# concatenate predictions and convert true_labels to numpy array\n",
    "predictions = torch.cat(predictions).cpu().numpy()\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# save evaluation results to CSV\n",
    "results_df = pd.DataFrame({'true_labels': true_labels, 'predicted_labels': predictions})\n",
    "results_df.to_csv(f'{project_dir}/model_output/results_{model_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "29967b79-2d62-45e1-a01e-15d5fd00919a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3680, 2)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1c25957d-c2e7-4734-b989-12ab766cbc20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2369"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.sum(results_df['true_labels'] == results_df['predicted_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "19239075-b328-419b-9b38-9afd8f39b64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64375"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2369/3680"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4fcbf1a-9b93-4f1a-952e-498f922a04dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[2100]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "193f4cff-5745-4e0c-a85a-ce1e6d3514b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,    5,   47,  151, 1519,  120, 1178, 3648,   33,  357,   29,  584,\n",
       "        4022,  268,   28,  139,   34,  131,  255, 2191,  830,  115,  130,   41,\n",
       "          31,  381,   20,  118, 2940,  427,   84,   61, 1340,   41,  875,  454,\n",
       "          47,  113,  663,   47,   45,   89,  851,  105,   26,  196,  392,  357,\n",
       "         404,  246,   65, 2870, 1817,  655,   46, 3334,  457,  380, 1113,   28,\n",
       "         673, 3765,  188,  820,  425,   10,   64,  950,  119,  985, 1372,   88,\n",
       "         621, 1230,   30,  120, 2044,  211,  123,   28,   19,  330,  244,   62,\n",
       "        3910,  210,   82,   64,  162,  118,   75,  206,  190,  353,  405,   69,\n",
       "          47,  399, 1682,   22,  920,   19,    2,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0], device='cuda:0')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[2000]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d1f92eb2-43ee-4123-8be1-6506b68c4c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCCGTTATTACTACCGCTGAAACCAAACATGTGATCAATTGGTCAACAAGGTATGTCTTTTGTTACTAACGAGCGTTTCTGGAATAGTTTGAACGGGATAATCCTCGATTTAGTCTTTCAGTCCCTTTGGCAAAAAGTTGTTTACATCGACTTGGAGAGTTGGACTTTTGCTTTTACTCATTTCAAGAAACAAAATTTTTGAAAATAATCCATTTCGCGTACGTTCAAAATGATATAATTACACATTGGCAACAATTATTTAGATTATATCACGTAGCTATTGTACTATTTTCCTATGAGAGAACTGCTGTAGCCATGTGGGATGATCATGCGGGCAAAACTAAAATACTTATATCAGTCCACAATTGAAGTATGGAAAGTTTTCTCCCAACTATGCCGTTTTTATAATGGAGACTAATGTCAAAGATAGGTTTTGCAGGCTTGATGGGTTTTCAAGACACTTTTCGGTCAAGATTTTCAGTGAGATTTGAAATTCTTG\n",
      "500\n",
      "1\n",
      "{'input_ids': tensor([[   1,    5,   13,  495, 1148,   13,  978,  131,   65,   23,  379, 1153,\n",
      "          303,   70,  167,   77,   79,   93,  764, 1532,  237,   52,   72,  250,\n",
      "           26, 1272,   14,  161, 2897,  157,  707,  395,   80,   16,  226,   33,\n",
      "          222,   33,   91, 1198,  432,   56,   45,   96,  311, 3921,  356,  685,\n",
      "           35,  134,   96,   23, 3939,  352,  328,  581,   94,  260,  211,  194,\n",
      "          612,   35,  854,  114,   23,  145,  374, 1991,   42,  914,  224,   72,\n",
      "          291,  280, 1178,  211,  553,  162,   25,  312,   45,   87,   78,  961,\n",
      "          139,   87, 1981,   50,  806,  107, 1462,   21,  253, 2660,  568, 2707,\n",
      "           91,   72,   56,  284,  721,   73, 1079,   29,    7,    2,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   1,    5,   13,  495, 1148,   13,  978,  131,   65,   23,  379, 1153,\n",
       "          303,   70,  167,   77,   79,   93,  764, 1532,  237,   52,   72,  250,\n",
       "           26, 1272,   14,  161, 2897,  157,  707,  395,   80,   16,  226,   33,\n",
       "          222,   33,   91, 1198,  432,   56,   45,   96,  311, 3921,  356,  685,\n",
       "           35,  134,   96,   23, 3939,  352,  328,  581,   94,  260,  211,  194,\n",
       "          612,   35,  854,  114,   23,  145,  374, 1991,   42,  914,  224,   72,\n",
       "          291,  280, 1178,  211,  553,  162,   25,  312,   45,   87,   78,  961,\n",
       "          139,   87, 1981,   50,  806,  107, 1462,   21,  253, 2660,  568, 2707,\n",
       "           91,   72,   56,  284,  721,   73, 1079,   29,    7,    2,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0], device='cuda:0'),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'),\n",
       " 'labels': tensor([1], device='cuda:0')}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df41ad3-e43d-44d3-adc0-ee92f2c93e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9755e22d-b6b7-4391-a461-49c151412083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f988f1ba-8917-4afd-bc36-f7f698d01c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189efbae-3489-44a1-81a7-01ed8cc926fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b247a456-2a55-4fc2-bc1b-81a2187a20fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d566c85b-cb2d-4f5a-b7bd-a9ce83b1adc0",
   "metadata": {},
   "source": [
    "### test sequence for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1dd540d4-ec6e-4499-9aa8-e26d2ca50e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    AutoModelForSequenceClassification, \n",
    "    DataCollatorForTokenClassification, \n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from transformers.models.bert.configuration_bert import BertConfig \n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\")\n",
    "#model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, config=config)\n",
    "#model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, padding=True)\n",
    "\n",
    "sequence = 'CTCTTTTTTGATGCGAATACCTGTTGTGGCGGAAACTGAAGTCCCTGCCCATCAGCTGTTCGACGAGTTTGCGTTGTTCTGCGGCAGGCGTTGACATAGTTGACATTCTGACGGTAGTTGGCAGCTTTCTGCTGGGAGTGTAGAGCCTTTTGTAGTATAACTTTTTGTTCTTTTCTTTCTTCTCTAATGCCTGAGGCTTCGGCGGTTATTCGGGTAATACATTAAGGAAGTTGCCATGATGTGGAAGAATACGACTAGTCAGTTAGCGATGGCCAGCTCCTTTACCTAAATCATGTGGCCTATCTTCAGATAGCATACTACCACCAACCATCAATACCTCAATGGCTTTCAACAAGTACCCTTCGTCGGGTTTTCAGCTTTTTTCCTACTTTGCTGGATGCTTTGTGAAGATCAGGAAGAAATTTTATCGGAAAAACCTCACTAGGAGAAACTATATAAAAGAAGGGTCAGACTGAAAATGACGCCGTAATAAACTCGTG'\n",
    "label = 1\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9814909f-a559-4f1e-a74a-d8049cf39e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9649deec-4250-427c-9571-c941ff2160ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9ab8805b-bd8f-47c8-a2a8-6ce8b0d053b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9527,  0.9219]], device='cuda:0'), hidden_states=tensor([[[-0.1754, -0.0160,  0.1625,  ...,  0.0359,  0.2485,  0.2095],\n",
       "         [ 0.2011, -0.0184, -0.0584,  ...,  0.2842,  0.0031, -0.1131],\n",
       "         [-0.2527, -0.0199,  0.0328,  ...,  0.0667,  0.1246,  0.1658],\n",
       "         ...,\n",
       "         [-0.0112, -0.1188,  0.1362,  ...,  0.2800,  0.0241, -0.1264],\n",
       "         [ 0.2483,  0.5387,  0.0147,  ..., -0.1258,  0.1126,  0.1182],\n",
       "         [-0.1736,  0.0155,  0.0341,  ..., -0.0446,  0.1998,  0.3108]]],\n",
       "       device='cuda:0'), attentions=None)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c08479-ca6b-41be-88cc-b2b7eff321bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna [~/.conda/envs/dna/]",
   "language": "python",
   "name": "conda_dna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
