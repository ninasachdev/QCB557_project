{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60a65e5-4d03-41a4-9fe3-4731012cad99",
   "metadata": {},
   "source": [
    "# Fine-tuning script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d251927-6ec1-4a38-ae01-5cfb2b45fb55",
   "metadata": {},
   "source": [
    "*Anusha, Joyce, Nina*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459bf7cb-8cb3-49b8-9d1c-98ff46686ffd",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6f60bbc-a2e8-4ad3-acdf-0ceba0a38fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "import csv\n",
    "from Bio import SeqIO\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89d38251-2066-40f6-84f3-c4412cba2e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = '/scratch/gpfs/ns5404/QCB557_project'\n",
    "\n",
    "model_out_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89162e24-9c9d-4777-8310-3927638dacce",
   "metadata": {},
   "source": [
    "#### Load model and tokenizer from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f9826f6a-6d87-47ee-8778-d9700397329a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# use gpu\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#model.to(device)\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\")\n",
    "config.num_labels #2 labels\n",
    "model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, config=config).to(device)\n",
    "\n",
    "# don't need to move the tokenizer to gpu b/c it's light\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "tokenizer.pad_token = \"X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2c6ced3-c989-41b6-8d1d-f40f1d8bbdd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "024f4262-11eb-4aae-81f2-62c264d798ea",
   "metadata": {},
   "source": [
    "#### Freeze gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0f61d91-4d77-479c-b3e3-b622d5808ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze the last layer in the encoder block\n",
    "for name, param in model.named_parameters():\n",
    "    if \"encoder.layer.11\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e1a0cf-f338-48b1-bc64-8eb9be02d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change classification head of the model\n",
    "#reference: https://discuss.huggingface.co/t/how-do-i-change-the-classification-head-of-a-model/4720/3\n",
    "class BinaryDNABERT2Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryDNABERT2Model, self).__init__()\n",
    "\n",
    "        self.base_model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, config=config).to(device)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(768, 2) # output features from bert is 768 and 2 is number of labels\n",
    "        \n",
    "    def forward(self, input_ids, attn_mask):\n",
    "        outputs = self.base_model(input_ids, attention_mask=attn_mask)\n",
    "        # You write you new head here\n",
    "        outputs = self.dropout(outputs[0])\n",
    "        outputs = self.linear(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17692c61-4ad2-46a6-9341-cb2293c01bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertUnpadAttention(\n",
      "          (self): BertUnpadSelfAttention(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (mlp): BertGatedLinearUnitMLP(\n",
      "          (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "          (act): GELU(approximate='none')\n",
      "          (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f4bca-6dc8-4898-a2b7-9930fc378101",
   "metadata": {},
   "source": [
    "#### Load data from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9a8ad43-cd08-45f6-8ff8-e6698343d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_data_load(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, shuffle=True):\n",
    "        if shuffle:\n",
    "            self.dataframe = dataframe.sample(frac=1).reset_index(drop=True)  # shuffle the dataframe\n",
    "        else:\n",
    "            self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.dataframe.iloc[idx]['sequence']\n",
    "        label = self.dataframe.iloc[idx]['label']\n",
    "\n",
    "        # tokenize the sequence\n",
    "        # tokenizer automatically generates attention masks\n",
    "        #DNABERT-2 max length = 128?\n",
    "        inputs = self.tokenizer(sequence, padding='max_length', max_length=128, truncation=True, return_tensors='pt')\n",
    "        \n",
    "        # move inputs to gpu\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a25f40c0-d544-4985-80fb-d65094b9c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_to_csv(fasta_file, csv_file):\n",
    "    with open(fasta_file, 'r') as fasta_fh, open(csv_file, 'w', newline='') as csv_fh:\n",
    "        writer = csv.writer(csv_fh)\n",
    "        writer.writerow(['sequence', 'label'])\n",
    "        for record in SeqIO.parse(fasta_fh, 'fasta'):\n",
    "            label = record.id.split('|')[-1]\n",
    "            sequence = str(record.seq)\n",
    "            writer.writerow([sequence, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "597f3041-4074-439c-a8dd-156c58537293",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_file = f'{project_dir}/data/H3K4me3/train.fna'\n",
    "csv_file =f'{project_dir}/data/train.csv'\n",
    "fasta_to_csv(fasta_file, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78e2826e-709f-4b1e-8bf5-7bf746b72678",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_file = f'{project_dir}/data/H3K4me3/test.fna'\n",
    "csv_file = f'{project_dir}/data/test.csv'\n",
    "fasta_to_csv(fasta_file, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b05259-1203-4d4d-b9eb-56dd3b5cea1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0da922b8-534d-4773-a9fe-ac32e9b9eb32",
   "metadata": {},
   "source": [
    "#### load in train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc6ed7f8-af00-4d7d-b63f-3c9f7fcf2a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(f'{project_dir}/data/train.csv')\n",
    "train_ds = custom_data_load(dataframe = train_data, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "83c0ca0d-f531-435e-bfcc-a46d801cd68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(f'{project_dir}/data/test.csv')\n",
    "test_ds = custom_data_load(dataframe = test_data, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5b856f2-0b5d-4d19-bdd5-b91916061b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   1,   41, 1019,   25, 1518,   14,  409, 1132,   42,   37,  444,   29,\n",
       "          119,  129,  183,  391,  504,  147,   23,  127,   43,  265,  121,  194,\n",
       "           33,  952,   33,  700,  202,  104,  159,   44,   94,   43,  140, 1641,\n",
       "          340,  930, 2081,  299, 3896,  860,  262,  116,  364,  114,  384,   47,\n",
       "          267, 1676,   70,  170,   45,  331,  114,  102,   29,  196,   72, 1655,\n",
       "          660,   22,  105,   36, 1460,   35,  172,  712, 1500,  358, 3780,  590,\n",
       "          495,  315,   36, 1492, 3527, 2139,  435,   38,   16,  250,   55,  703,\n",
       "          443,   21,  674,   35,  732, 3754,   43,  232,  252,   36,  353, 2559,\n",
       "          273,  318, 3537,  124,   60, 1760,    7,    2,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0], device='cuda:0'),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'),\n",
       " 'labels': tensor(1)}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da355e56-064b-4625-ba54-1aeaa748be55",
   "metadata": {},
   "source": [
    "#### training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8aa17d-a48b-47bb-a685-413713e56bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        per_device_train_batch_size=16, #powers of 2\n",
    "        gradient_accumulation_steps=2,\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_num_workers=4,\n",
    "        \n",
    "        fp16=True, \n",
    "        weight_decay=0.015, \n",
    "        num_train_epochs=2, \n",
    "        log_level=\"error\",\n",
    "        output_dir=model_out_dir,\n",
    "        evaluation_strategy=\"steps\",  \n",
    "        eval_steps=800, \n",
    "         save_strategy=\"epoch\",        \n",
    "         logging_steps=800,\n",
    "         logging_strategy=\"steps\",\n",
    "         logging_dir=logging_dir,\n",
    "         overwrite_output_dir = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242bb20-c06f-4a4a-8bcd-d10a9f05194a",
   "metadata": {},
   "source": [
    "#### metrics to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3fd445-39f5-48e9-9834-a7d32e194b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#or can define custom metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d5904-01d8-4d7d-823b-57edbb682abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd838666-2126-4080-a63c-849c2d97af5b",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f8d8e-dea0-4a9e-b72c-c1aff50508c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env [~/.conda/envs/torch-env/]",
   "language": "python",
   "name": "conda_torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
