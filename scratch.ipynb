{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cafe753-7b5a-4cb3-9d4c-30b8d57eeda7",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4144e3-13b7-46b6-bdc8-4f2a4a3e8073",
   "metadata": {},
   "source": [
    "### Sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68defd6-9920-4731-9253-3b20359a78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40795e07-67e1-4151-afe2-78c12c25ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/scratch/gpfs/aa8417/QCB557_project/data/test.csv')\n",
    "train = pd.read_csv('/scratch/gpfs/aa8417/QCB557_project/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73276dd-149a-45ac-9251-fcda0345dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seqs = test['sequence'].tolist()\n",
    "train_seqs = train['sequence'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35353be-ea46-4c3c-9136-69c41571f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = test_seqs + train_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de617a8-c060-4b7c-a587-effa2109ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_lengths(sequences):\n",
    "    seq_lengths = []\n",
    "    for sequence in sequences:\n",
    "        length = len(sequence)\n",
    "        seq_lengths.append(length)\n",
    "    return seq_lengths\n",
    "\n",
    "lengths = get_seq_lengths(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab75520-70f5-426c-ba91-de50b4fb5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lengths, bins=20)\n",
    "plt.xlabel('length of DNA sequences in train and test set')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('histogram of DNA sequence lengths')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b407f-e56e-43fe-861b-96ec03a16657",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5dec6-649f-489b-8891-22a772cf5458",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceecee31-46e9-49cf-a85c-b4ceeaf1debd",
   "metadata": {},
   "source": [
    "So, sequence lengths range from 290 to 500. So max_length in tokenizer should be 500."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69611edc-b11d-476f-8b06-763ef6272a50",
   "metadata": {},
   "source": [
    "### Dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f650d47-be0c-4cc2-b389-ee72129c8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dbde974-f9da-45b6-8016-e979c9ba6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "histones = ['H3', 'H3K14ac', 'H3K36me3', 'H3K4me1', 'H3K4me2', 'H3K4me3', 'H3K9ac', 'H4', 'H4ac']\n",
    "\n",
    "percents_0 = []\n",
    "percents_1 = []\n",
    "totals = []\n",
    "\n",
    "data_dict = {}\n",
    "\n",
    "\n",
    "for i in range(9):\n",
    "    histone = histones[i]\n",
    "    train = pd.read_csv(f'/scratch/gpfs/aa8417/QCB557_project/data/{histone}/train.csv')\n",
    "    test = pd.read_csv(f'/scratch/gpfs/aa8417/QCB557_project/data/{histone}/test.csv')\n",
    "    \n",
    "    df = pd.concat([train, test], ignore_index=True)\n",
    "    \n",
    "    label_counts = df['label'].value_counts(normalize=True) * 100\n",
    "    percent_0 = label_counts[0]\n",
    "    percent_1 = label_counts[1]\n",
    "\n",
    "    rows, _ = df.shape\n",
    "    \n",
    "    percents_0.append(percent_0)\n",
    "    percents_1.append(percent_1)\n",
    "    totals.append(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb9a2868-6038-4992-b77f-f047a535dc63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48.76712328767123,\n",
       " 43.200798838053736,\n",
       " 45.83715596330275,\n",
       " 45.493575780534776,\n",
       " 40.86953687709807,\n",
       " 46.726813228620344,\n",
       " 44.514433806061476,\n",
       " 55.61947811793713,\n",
       " 46.00381287578824]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percents_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea17b8ce-0236-41fc-b6ad-617a78c2f74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51.23287671232877,\n",
       " 56.79920116194626,\n",
       " 54.162844036697244,\n",
       " 54.50642421946523,\n",
       " 59.13046312290193,\n",
       " 53.273186771379656,\n",
       " 55.48556619393852,\n",
       " 44.38052188206287,\n",
       " 53.99618712421176]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percents_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76f8c222-bddc-4185-bffe-8aef82874919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H3',\n",
       " 'H3K14ac',\n",
       " 'H3K36me3',\n",
       " 'H3K4me1',\n",
       " 'H3K4me2',\n",
       " 'H3K4me3',\n",
       " 'H3K9ac',\n",
       " 'H4',\n",
       " 'H4ac']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfdf1e6e-a349-4171-a088-901851816096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14965, 33048, 34880, 31677, 30683, 36799, 27782, 14601, 34095]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b05d4f-8ab2-468e-ac40-9d722b91cb2b",
   "metadata": {},
   "source": [
    "### Code bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a7741-42f7-4214-be01-02ec68a7dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DON'T NEED THIS ANYMORE\n",
    "#change classification head of the model\n",
    "#reference: https://discuss.huggingface.co/t/how-do-i-change-the-classification-head-of-a-model/4720/3\n",
    "class BinaryDNABERT2Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryDNABERT2Model, self).__init__()\n",
    "\n",
    "        self.base_model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, config=config).to(device)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(768, 2) # output features from bert is 768 and 2 is number of labels\n",
    "        \n",
    "    def forward(self, input_ids, attn_mask):\n",
    "        outputs = self.base_model(input_ids, attention_mask=attn_mask)\n",
    "        # You write you new head here\n",
    "        outputs = self.dropout(outputs[0])\n",
    "        outputs = self.linear(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c093a-95f6-4435-b1ce-4e034827169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.Tensor(labels).long()\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083acae-9d0d-4730-a4ce-af52a9cc55e0",
   "metadata": {},
   "source": [
    "## Attention weight stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02882fb6-20bd-4902-8674-667e6b9dc6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aa8417/.conda/envs/dna/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    AutoModelForSequenceClassification, \n",
    "    DataCollatorForTokenClassification, \n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from transformers.models.bert.configuration_bert import BertConfig \n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from training_args_module import training_args\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "import csv\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ddf35f1-91db-4c06-8b27-84fbfe0a2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "princeton_id = 'aa8417'\n",
    "project_dir = f'/scratch/gpfs/{princeton_id}/QCB557_project'\n",
    "\n",
    "model_name = 'fine_tune_new_v3'\n",
    "model_out_dir = f'{project_dir}/models/{model_name}'\n",
    "\n",
    "# use gpu\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4548c5b4-dd90-4f12-87f0-b2854bb8953e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(f'{model_out_dir}/config.json', output_attentions=True)\n",
    "print(config.num_labels) #2 labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_out_dir, trust_remote_code=True, config=config)\n",
    "model.to(device)\n",
    "'''\n",
    "config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\", output_hidden_states=True, output_attentions=True)\n",
    "print(config.num_labels) #2 labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, config=config)\n",
    "model.to(device)\n",
    "'''\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, padding=True)\n",
    "tokenizer.pad_token = \"X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ecbef32-c31b-40ac-bd59-de959853b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert config.output_attentions == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44b2d75e-485f-4577-84a4-0bdffcc660aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = 'TCAATCACTCTAAGAAGAAACAATCTAGAAAACTAATTAAATCGAGAAGACAAGATTTCACCACTAATGTAGAAATAAACCCTATTAACAAGAACTTGTATTTCCCACACATTTCTGCTGTTCAAATTTTTGACTTCTATAAAAATGAGCAAGTTAACTATCAGTATTTAACATCAGGTGTCAACAATTCTATGGGTAAAGTTAGATTTGAACTGAATTTACAAGACCCAATAATAACTGATTTGAAGTTCACCAAAGATGGGCAATGGATGATTACATACGAAATTGAGTATCCGCCAAATGACCTCTTATCTTCCAAGGACTTAACTCATATCTTGAAATTTTGGACCAAAAACGATAATGAGACAAATTGGAATTTGAAAACGAAAGTAATAAATCCACACGGGATAAGTGTCCCAATTACCAAGATATTGCCTTCACCAAGATCAGTTAATAATAGTCAAGGCTGTTTAACGGCTGACAACAACGGTGGACTGAAA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a18529da-a280-4ce9-b5ad-686c4d134fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1051, -0.0844, -0.0708,  ..., -0.1008,  0.3531, -0.0425],\n",
      "         [-0.0933,  0.1621, -0.1887,  ...,  0.3297,  0.3745,  0.6088],\n",
      "         [ 0.1130,  0.3378,  0.1361,  ..., -0.0580, -0.0935,  0.5449],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n",
      "()\n",
      "Predicted Label: 0\n",
      "Predicted Probabilities: tensor([[9.9968e-01, 3.2139e-04]], device='cuda:0')\n",
      "Attention Weights: None\n"
     ]
    }
   ],
   "source": [
    "def get_model_output(model, tokenizer, sequence, device):\n",
    "    inputs = tokenizer(sequence, padding='max_length', max_length=128, return_tensors='pt').to(device)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    #print(input_ids)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_attentions=True)\n",
    "        print(outputs.hidden_states)\n",
    "\n",
    "    predicted_probs = torch.softmax(outputs.logits, dim=-1)\n",
    "    attention_weights = outputs.attentions\n",
    "    print(np.array(attention_weights).shape)\n",
    "\n",
    "    predicted_label = torch.argmax(predicted_probs, dim=-1).item()\n",
    "\n",
    "    return predicted_label, predicted_probs, attention_weights\n",
    "\n",
    "predicted_label, predicted_probs, attention_weights = get_model_output(model, tokenizer, sequence, device)\n",
    "print(\"Predicted Label:\", predicted_label)\n",
    "print(\"Predicted Probabilities:\", predicted_probs)\n",
    "print(\"Attention Weights:\", attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f668c2-2d54-49e3-bf51-b585ef1163c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.output_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d30f8-47f6-436f-bf90-12ec9414c03d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209cdba-9e3a-4883-9af8-ad91087d1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d29bc9d-d5df-4b0c-977b-f743e3d18145",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d577b9b3-3764-4418-bbde-274343b2ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idntity = testing_indx_0/1\n",
    "sequence\n",
    "input_ids\n",
    "attention_weights\n",
    "embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8369f737-78e2-4830-a6cb-9670ace60dca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna",
   "language": "python",
   "name": "dna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
