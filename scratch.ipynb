{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cafe753-7b5a-4cb3-9d4c-30b8d57eeda7",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4144e3-13b7-46b6-bdc8-4f2a4a3e8073",
   "metadata": {},
   "source": [
    "Figure out how much the sequence length varies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68defd6-9920-4731-9253-3b20359a78ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40795e07-67e1-4151-afe2-78c12c25ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/scratch/gpfs/aa8417/QCB557_project/data/test.csv')\n",
    "train = pd.read_csv('/scratch/gpfs/aa8417/QCB557_project/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73276dd-149a-45ac-9251-fcda0345dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seqs = test['sequence'].tolist()\n",
    "train_seqs = train['sequence'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35353be-ea46-4c3c-9136-69c41571f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = test_seqs + train_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de617a8-c060-4b7c-a587-effa2109ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_lengths(sequences):\n",
    "    seq_lengths = []\n",
    "    for sequence in sequences:\n",
    "        length = len(sequence)\n",
    "        seq_lengths.append(length)\n",
    "    return seq_lengths\n",
    "\n",
    "lengths = get_seq_lengths(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab75520-70f5-426c-ba91-de50b4fb5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lengths, bins=20)\n",
    "plt.xlabel('length of DNA sequences in train and test set')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('histogram of DNA sequence lengths')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b407f-e56e-43fe-861b-96ec03a16657",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5dec6-649f-489b-8891-22a772cf5458",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceecee31-46e9-49cf-a85c-b4ceeaf1debd",
   "metadata": {},
   "source": [
    "So, sequence lengths range from 290 to 500. So max_length in tokenizer should be 500."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b05d4f-8ab2-468e-ac40-9d722b91cb2b",
   "metadata": {},
   "source": [
    "### Code bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5a7741-42f7-4214-be01-02ec68a7dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DON'T NEED THIS ANYMORE\n",
    "#change classification head of the model\n",
    "#reference: https://discuss.huggingface.co/t/how-do-i-change-the-classification-head-of-a-model/4720/3\n",
    "class BinaryDNABERT2Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryDNABERT2Model, self).__init__()\n",
    "\n",
    "        self.base_model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, config=config).to(device)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(768, 2) # output features from bert is 768 and 2 is number of labels\n",
    "        \n",
    "    def forward(self, input_ids, attn_mask):\n",
    "        outputs = self.base_model(input_ids, attention_mask=attn_mask)\n",
    "        # You write you new head here\n",
    "        outputs = self.dropout(outputs[0])\n",
    "        outputs = self.linear(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79c093a-95f6-4435-b1ce-4e034827169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.Tensor(labels).long()\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083acae-9d0d-4730-a4ce-af52a9cc55e0",
   "metadata": {},
   "source": [
    "## Attention weight stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02882fb6-20bd-4902-8674-667e6b9dc6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aa8417/.conda/envs/dna/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    AutoModelForSequenceClassification, \n",
    "    DataCollatorForTokenClassification, \n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from transformers.models.bert.configuration_bert import BertConfig \n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from training_args_module import training_args\n",
    "import transformers\n",
    "import os\n",
    "\n",
    "import csv\n",
    "from Bio import SeqIO\n",
    "\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ddf35f1-91db-4c06-8b27-84fbfe0a2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "princeton_id = 'aa8417'\n",
    "project_dir = f'/scratch/gpfs/{princeton_id}/QCB557_project'\n",
    "\n",
    "model_name = 'fine_tune_new_v3'\n",
    "model_out_dir = f'{project_dir}/models/{model_name}'\n",
    "\n",
    "# use gpu\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4548c5b4-dd90-4f12-87f0-b2854bb8953e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /scratch/gpfs/aa8417/QCB557_project/models/fine_tune_new_v3 were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(f'{model_out_dir}/config.json', output_attentions=True)\n",
    "print(config.num_labels) #2 labels\n",
    "model = AutoModel.from_pretrained(model_out_dir, trust_remote_code=True, config=config)\n",
    "model.to(device)\n",
    "'''\n",
    "config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\", output_hidden_states=True, output_attentions=True)\n",
    "print(config.num_labels) #2 labels\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, config=config)\n",
    "model.to(device)\n",
    "'''\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, padding=True)\n",
    "tokenizer.pad_token = \"X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ecbef32-c31b-40ac-bd59-de959853b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert config.output_attentions == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44b2d75e-485f-4577-84a4-0bdffcc660aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = 'TCAATCACTCTAAGAAGAAACAATCTAGAAAACTAATTAAATCGAGAAGACAAGATTTCACCACTAATGTAGAAATAAACCCTATTAACAAGAACTTGTATTTCCCACACATTTCTGCTGTTCAAATTTTTGACTTCTATAAAAATGAGCAAGTTAACTATCAGTATTTAACATCAGGTGTCAACAATTCTATGGGTAAAGTTAGATTTGAACTGAATTTACAAGACCCAATAATAACTGATTTGAAGTTCACCAAAGATGGGCAATGGATGATTACATACGAAATTGAGTATCCGCCAAATGACCTCTTATCTTCCAAGGACTTAACTCATATCTTGAAATTTTGGACCAAAAACGATAATGAGACAAATTGGAATTTGAAAACGAAAGTAATAAATCCACACGGGATAAGTGTCCCAATTACCAAGATATTGCCTTCACCAAGATCAGTTAATAATAGTCAAGGCTGTTTAACGGCTGACAACAACGGTGGACTGAAA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a18529da-a280-4ce9-b5ad-686c4d134fbe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,   56, 2366,  451, 1304,   88,   85,   79,  364,   16,  145,  225,\n",
      "           73,  517,   79,  106, 1903,  338,   84,  466,   29,  160, 1608, 3076,\n",
      "          146, 2436,  554,   37,  125, 1838,  153, 1180,  160,   20,  197,  103,\n",
      "           56,  162, 3903, 3614, 3917,  119,   94,   27,  158,   27,  228,  459,\n",
      "           52,  448,  131,   83,  328,  108,  585,  104, 3403,  282,   26,  463,\n",
      "          528,  154,   47, 3918,  150,   63,  723, 2415,   33,  718,   93,   20,\n",
      "          199,  321,   57, 2597,  409,   68,   38,  401,   72,  250, 1094,  581,\n",
      "           76,  657, 2444,   76,  254,  153,  137,  168, 3658, 2846,   59,  361,\n",
      "           72,  108,  357,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]], device='cuda:0')\n",
      "(tensor([[[ 0.1051, -0.0844, -0.0708,  ..., -0.1008,  0.3531, -0.0425],\n",
      "         [-0.0933,  0.1621, -0.1887,  ...,  0.3297,  0.3745,  0.6088],\n",
      "         [ 0.1130,  0.3378,  0.1361,  ..., -0.0580, -0.0935,  0.5449],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0'), tensor([[ 1.9372e-01,  6.0635e-02,  1.0053e-01, -6.2175e-02, -7.3221e-02,\n",
      "          1.9701e-02, -2.5416e-01,  1.0592e-01, -2.0449e-01,  1.7441e-01,\n",
      "         -4.0919e-01,  7.1816e-02,  2.1010e-01, -3.2754e-03,  1.8307e-01,\n",
      "          1.0782e-01, -2.6469e-01, -1.5045e-01, -2.4260e-03, -2.7276e-01,\n",
      "          8.5655e-02,  9.1387e-03, -2.0887e-01,  1.5973e-01,  9.9774e-02,\n",
      "          2.9096e-01, -2.5948e-01,  8.1879e-02,  1.5911e-02, -4.2825e-02,\n",
      "          1.4608e-01, -2.6218e-01, -7.2016e-02, -6.0154e-02,  3.4734e-02,\n",
      "         -1.4667e-01, -3.3545e-01,  2.5795e-01, -3.4291e-02, -2.5025e-01,\n",
      "         -1.8134e-01, -1.6464e-01, -1.0921e-01, -1.2176e-01,  1.2451e-01,\n",
      "         -1.4881e-01, -1.3872e-02,  2.3110e-02,  2.5729e-01,  2.0666e-01,\n",
      "         -1.8172e-01,  1.1278e-01,  1.6506e-01, -1.1999e-01, -2.7081e-01,\n",
      "          2.0970e-01, -8.5578e-02, -9.1381e-03,  1.3466e-01,  2.0927e-02,\n",
      "         -4.1022e-03,  4.3163e-03,  3.1868e-01,  1.1898e-02, -6.2199e-02,\n",
      "          2.9027e-01, -1.1956e-01,  7.1314e-02, -2.1049e-01,  8.0153e-02,\n",
      "         -1.2821e-01, -4.7609e-03,  3.5111e-01, -2.2897e-02,  2.4223e-01,\n",
      "         -1.4233e-01,  2.1872e-01, -6.4689e-02, -5.3880e-02,  4.5964e-02,\n",
      "          2.1953e-01,  1.3798e-01,  2.4214e-01, -9.2764e-02, -2.4100e-01,\n",
      "         -9.4243e-02,  7.1841e-02, -3.0859e-01, -3.1761e-01, -9.8498e-02,\n",
      "         -6.4251e-02, -3.9920e-03, -1.8749e-01,  2.8243e-02, -2.2872e-01,\n",
      "         -1.2006e-01, -1.2506e-01, -1.0099e-01, -1.4935e-01, -2.5249e-01,\n",
      "         -2.1427e-02,  2.1106e-01, -1.4076e-01, -6.6438e-02, -6.3435e-02,\n",
      "         -3.5498e-01, -3.4260e-02,  1.0694e-01,  2.2184e-01, -1.7721e-01,\n",
      "          2.5353e-02, -2.3716e-01, -3.8402e-02, -1.8111e-01, -1.5255e-01,\n",
      "          1.4559e-01, -3.0850e-02, -2.2813e-02,  5.3879e-02,  4.2258e-02,\n",
      "         -6.5593e-02,  7.5644e-02,  2.2172e-01, -3.5416e-01, -2.9515e-01,\n",
      "          2.8038e-01,  5.9965e-02, -3.7366e-01,  1.5169e-01,  2.0173e-01,\n",
      "          2.2592e-02, -1.9739e-01, -1.1803e-01, -7.4192e-02,  2.1594e-01,\n",
      "         -3.0672e-01, -9.1367e-02,  6.2567e-02, -2.6809e-02,  1.9898e-01,\n",
      "         -9.4762e-02,  2.0187e-01, -1.0516e-01,  2.2641e-01, -2.0913e-01,\n",
      "         -1.1560e-01, -3.6866e-02,  2.6115e-01, -1.2163e-01, -3.8054e-01,\n",
      "         -4.6802e-01, -2.1851e-01, -2.2733e-01, -2.1085e-02, -1.1561e-01,\n",
      "          2.4805e-01,  2.8685e-01, -2.2181e-01, -2.9737e-01, -9.4972e-02,\n",
      "          1.9636e-01,  2.2541e-01, -8.4266e-02, -1.0489e-01, -1.7468e-02,\n",
      "         -3.4334e-02,  1.9135e-01, -2.3049e-01, -2.4051e-01,  1.7584e-01,\n",
      "          3.0102e-01, -1.0174e-01, -1.0480e-01,  1.2232e-01, -5.1294e-03,\n",
      "         -1.6111e-01, -2.2748e-01,  9.6890e-02, -1.3533e-01,  9.6969e-02,\n",
      "          2.6936e-02,  1.2698e-01, -1.0104e-01, -9.0972e-02, -3.7396e-03,\n",
      "         -5.0941e-02,  2.2622e-01,  1.4498e-01,  8.1393e-02,  1.9333e-01,\n",
      "         -1.5295e-02, -4.3610e-02, -3.2724e-01,  7.0045e-02,  1.3689e-01,\n",
      "          5.5042e-02, -1.3407e-01, -1.0529e-01, -1.0018e-01, -1.3049e-01,\n",
      "          1.5372e-01, -2.0660e-01, -2.4296e-02, -1.2030e-01, -7.7516e-02,\n",
      "         -1.1217e-01, -6.6587e-02,  2.0292e-01,  5.4012e-02,  8.4487e-02,\n",
      "         -1.6717e-01, -3.9436e-04, -1.1079e-01, -2.8773e-01, -1.4497e-01,\n",
      "         -4.4537e-02,  3.5101e-01, -9.1519e-02,  6.9051e-02,  5.7106e-02,\n",
      "          3.6782e-01,  3.9310e-02,  4.9852e-02,  1.6769e-01, -3.6557e-02,\n",
      "         -2.7029e-01, -5.4969e-04,  6.1256e-02,  4.9921e-01,  3.5625e-02,\n",
      "         -1.5251e-01,  1.0432e-01, -2.8232e-02,  1.9903e-01,  1.5375e-01,\n",
      "         -9.1592e-02,  2.1909e-01,  2.3973e-01,  1.5327e-01,  3.7516e-02,\n",
      "          1.9323e-01,  1.0190e-01,  1.4971e-01,  2.9451e-01,  1.2538e-01,\n",
      "         -1.7095e-01,  1.3630e-01,  1.6553e-01,  1.4408e-01, -1.6954e-01,\n",
      "          5.1890e-02,  2.4392e-01, -1.0221e-01,  2.2948e-01,  1.2630e-01,\n",
      "          6.5473e-02, -2.3859e-01,  2.6757e-01, -1.2797e-01, -4.7404e-02,\n",
      "         -3.7209e-01,  1.3932e-01,  7.7607e-02,  7.4536e-02, -1.8313e-01,\n",
      "          1.7651e-01,  2.0114e-02,  2.3074e-01, -3.1348e-02, -5.7026e-02,\n",
      "          2.5049e-01,  1.6153e-01,  1.8470e-02,  5.8303e-02,  1.7059e-01,\n",
      "          1.8608e-01, -1.8535e-01,  3.4977e-02,  1.0419e-01,  1.7230e-01,\n",
      "         -1.1046e-01, -1.6249e-01,  6.5594e-02, -1.4085e-02, -6.3126e-02,\n",
      "         -3.1925e-02, -1.0321e-01, -2.0238e-01,  4.4737e-02, -3.4780e-01,\n",
      "         -1.1829e-01, -1.7397e-01,  7.4418e-02,  3.9616e-02,  1.0530e-02,\n",
      "         -3.8293e-01, -2.3583e-02, -1.7748e-01,  7.6875e-02, -2.5153e-01,\n",
      "          1.9022e-01,  1.5572e-01, -1.2500e-01, -1.8419e-01,  2.2326e-01,\n",
      "          2.8673e-01, -2.6442e-01,  2.8485e-03,  1.4091e-01, -8.4016e-03,\n",
      "          4.2179e-02,  2.5397e-01,  2.4793e-01,  1.0484e-01, -1.5327e-01,\n",
      "         -5.0016e-03, -9.0287e-02,  1.4395e-01,  2.0537e-02,  4.8886e-02,\n",
      "          5.1047e-02, -2.0333e-01,  2.6523e-01, -2.4271e-02, -1.0276e-01,\n",
      "          2.3798e-01,  9.1055e-02,  2.7791e-01,  7.9537e-04,  1.4624e-01,\n",
      "         -1.3961e-01, -1.5029e-01, -5.0393e-02,  1.7454e-01,  2.2673e-01,\n",
      "          3.0496e-01,  1.0077e-02,  8.3591e-02, -1.4109e-01, -1.3386e-01,\n",
      "          3.5144e-02,  4.7433e-02, -1.7243e-01,  3.6082e-01,  1.7975e-02,\n",
      "         -1.1265e-01, -3.2691e-01,  1.9084e-02,  1.8099e-01, -2.6218e-01,\n",
      "         -8.6036e-02,  1.0964e-01, -4.3786e-02, -9.3185e-02, -1.7700e-02,\n",
      "          6.6216e-02,  2.8512e-01, -2.9974e-02, -2.2872e-02,  2.0090e-02,\n",
      "          2.6104e-01, -1.4725e-01, -2.5076e-02,  3.8718e-03, -7.3986e-02,\n",
      "         -3.3222e-01, -1.3326e-03,  6.4274e-02,  2.4425e-01,  1.0259e-01,\n",
      "          1.1620e-01,  1.0866e-01, -1.0517e-01, -4.0552e-01,  1.8881e-02,\n",
      "         -3.0637e-02,  1.5201e-02,  2.5019e-01, -1.1193e-01,  3.6792e-01,\n",
      "         -3.5986e-01, -6.0918e-02, -2.8267e-01, -1.5502e-01,  3.4988e-01,\n",
      "         -7.9508e-02, -1.0834e-01,  1.0998e-01,  1.7438e-02,  1.5664e-02,\n",
      "          3.5887e-02, -2.3492e-01,  2.9225e-02,  1.2384e-01, -3.6124e-01,\n",
      "         -1.4187e-01,  3.8811e-01, -4.5744e-02, -6.3226e-02, -3.1393e-02,\n",
      "          2.2379e-01,  1.0575e-01,  2.6398e-02,  3.2280e-01,  1.0194e-02,\n",
      "         -1.0094e-01,  2.3661e-01,  4.6661e-01,  4.1123e-01, -1.8521e-01,\n",
      "         -2.8256e-01, -5.4077e-02, -9.2112e-02, -1.0612e-01, -2.6762e-03,\n",
      "          2.3174e-01, -8.6199e-02,  1.5009e-01,  8.2172e-03,  1.3031e-01,\n",
      "         -7.6337e-02, -1.3817e-01,  1.3969e-01, -1.8408e-02, -1.3581e-02,\n",
      "          1.3064e-01, -7.7627e-03, -1.4434e-01,  2.6280e-01,  3.8500e-02,\n",
      "          9.1614e-02,  2.1505e-02, -2.4823e-01,  1.5094e-01,  3.7808e-02,\n",
      "          1.6415e-01,  4.3016e-02,  1.1024e-02,  2.5363e-01, -1.5410e-01,\n",
      "         -5.2690e-02,  1.3033e-01,  5.6900e-02,  2.7426e-01, -1.4996e-01,\n",
      "          3.3958e-02,  2.5329e-01,  4.6411e-02,  2.2397e-01,  1.6364e-01,\n",
      "         -3.0180e-01,  2.7096e-01,  2.6170e-01,  2.6844e-02,  4.6294e-02,\n",
      "         -5.8977e-02,  1.7636e-01, -7.1792e-02,  2.5755e-01,  2.1122e-01,\n",
      "         -1.3776e-01, -9.4748e-04, -1.1708e-01,  1.1793e-02,  1.3793e-01,\n",
      "         -1.5317e-01, -1.2370e-01, -1.2070e-02, -8.8699e-02, -1.1893e-01,\n",
      "          2.4317e-01,  5.6973e-02,  2.9864e-02,  1.6207e-01,  1.2357e-01,\n",
      "          7.7187e-02,  2.2431e-01, -3.0838e-01,  2.3059e-01,  3.0949e-01,\n",
      "          1.5479e-01, -1.9488e-01,  1.0021e-01, -2.3662e-02,  3.8178e-02,\n",
      "         -1.9017e-01, -2.4456e-01, -2.8974e-02,  1.9167e-01,  6.8778e-03,\n",
      "         -4.0678e-02,  2.5869e-02,  3.5665e-02,  3.2524e-01,  2.2059e-01,\n",
      "          3.8268e-01,  5.9165e-03,  3.5264e-01, -7.4525e-02, -1.6457e-01,\n",
      "         -6.3048e-02,  6.7801e-02,  4.3323e-01, -1.7649e-01,  3.9365e-01,\n",
      "         -2.5362e-02,  4.5790e-02, -2.9136e-01, -1.0487e-01,  1.7049e-01,\n",
      "         -1.1209e-01, -3.4471e-01, -1.7115e-02,  8.6249e-02, -1.2238e-01,\n",
      "          2.6157e-01, -3.3717e-01,  1.6715e-01, -1.6530e-03,  3.2854e-02,\n",
      "          2.0305e-01,  4.9038e-02, -1.2876e-01, -2.0401e-01, -2.8627e-01,\n",
      "         -2.3460e-01, -1.2855e-01,  3.2623e-02, -1.2891e-01, -1.7498e-01,\n",
      "          5.2070e-02, -7.6364e-02, -3.4848e-01, -3.1128e-01,  2.0077e-01,\n",
      "          4.1537e-01, -2.6896e-01,  2.6175e-01, -3.4019e-01,  1.4357e-02,\n",
      "          3.9209e-01,  2.2140e-01,  4.7866e-02, -4.3448e-02, -8.2912e-02,\n",
      "         -3.1483e-02,  3.5109e-02, -9.2000e-02, -3.6829e-02,  6.2102e-02,\n",
      "          9.9723e-02,  4.8959e-02, -4.4789e-01,  7.8225e-02, -8.3071e-02,\n",
      "         -1.4556e-02, -6.6891e-02, -2.4018e-01, -1.3347e-01,  7.1346e-02,\n",
      "          3.3084e-01,  6.6623e-02,  1.5519e-02,  1.1757e-01,  2.0477e-01,\n",
      "         -1.4566e-01,  2.2561e-02, -2.3647e-02,  6.5015e-02, -3.1053e-01,\n",
      "         -5.7726e-02,  2.1799e-01, -3.9689e-01, -1.7606e-01,  2.2156e-01,\n",
      "         -5.8672e-02,  1.8435e-01, -3.1669e-01,  5.9610e-02, -2.9887e-01,\n",
      "          2.0560e-01, -8.7587e-03,  1.5336e-01, -7.6479e-02,  1.0696e-01,\n",
      "          1.0047e-01,  1.8620e-01,  9.2122e-02,  1.7869e-02,  4.3194e-03,\n",
      "         -3.2601e-01, -1.7190e-02,  3.2583e-01,  6.1703e-02,  3.0021e-01,\n",
      "         -1.5219e-01,  1.1427e-01, -1.3447e-01,  1.5316e-02,  8.0341e-02,\n",
      "         -3.6923e-01, -1.2122e-01,  2.1211e-01, -1.7884e-01,  2.8646e-02,\n",
      "         -2.0610e-01,  3.2987e-01,  4.9546e-02,  8.5459e-02, -2.3576e-01,\n",
      "          7.0084e-02, -8.9290e-02,  4.0563e-02, -4.2070e-02, -2.8968e-01,\n",
      "          1.2991e-01,  1.1919e-01, -2.7962e-01,  1.1638e-01, -1.5637e-01,\n",
      "         -2.9127e-01,  8.5063e-02,  7.6897e-02,  3.6539e-01, -1.1606e-01,\n",
      "         -7.0381e-02, -9.1831e-02,  1.0277e-01,  2.0362e-01,  1.8292e-01,\n",
      "          1.9755e-02, -5.1247e-02, -7.0289e-02, -8.8028e-02, -1.5045e-02,\n",
      "          9.2867e-02, -2.9166e-02,  9.5269e-02,  1.2307e-01,  5.0997e-02,\n",
      "         -1.2525e-02,  3.3061e-02,  2.4623e-01,  1.2148e-01,  2.7363e-01,\n",
      "         -2.1651e-02, -1.9701e-01, -1.8016e-01, -1.0935e-01, -3.8211e-02,\n",
      "          1.9926e-03,  7.5998e-02, -6.1148e-02, -2.8258e-02,  6.4005e-02,\n",
      "          1.0302e-01,  1.7369e-01, -2.4789e-02, -5.9880e-02, -6.6965e-02,\n",
      "         -8.6172e-02,  2.2702e-01, -5.7155e-02, -2.1051e-01,  8.6397e-02,\n",
      "         -1.3310e-01, -2.2043e-02, -1.0227e-01,  1.3167e-03, -2.1161e-01,\n",
      "         -5.8307e-02, -2.1800e-02, -6.8184e-02, -3.0663e-01, -1.4883e-01,\n",
      "          6.5798e-02, -1.9932e-02,  1.8693e-01,  7.2544e-02, -5.8532e-02,\n",
      "         -2.5317e-01, -1.3820e-01,  8.7012e-03, -3.0616e-02,  8.7434e-02,\n",
      "          5.2191e-02,  1.6373e-01,  1.3782e-02, -9.9077e-02, -2.5184e-02,\n",
      "         -2.4929e-01, -3.1503e-01, -1.3450e-01, -2.4564e-01, -2.6959e-01,\n",
      "         -1.6878e-01,  1.9377e-02, -2.1888e-01, -2.8454e-01, -1.3726e-01,\n",
      "         -7.1082e-02,  1.9425e-01,  6.4675e-02,  2.1649e-01,  1.7206e-01,\n",
      "         -3.5785e-02, -8.3528e-02, -7.9681e-02,  1.9968e-01, -2.1628e-02,\n",
      "         -2.1656e-01,  1.3036e-02, -1.9260e-01, -3.1138e-01,  1.9017e-01,\n",
      "          4.3022e-02,  9.7132e-02, -7.8364e-02, -1.5768e-01,  2.3732e-02,\n",
      "         -1.8757e-01,  1.9772e-01,  1.4828e-01,  1.4896e-01,  3.7708e-01,\n",
      "         -1.1847e-01, -9.6461e-02, -1.3218e-01,  4.8384e-02, -8.3383e-02,\n",
      "         -1.0264e-01,  3.5647e-02, -1.7426e-01, -5.5651e-02,  4.7788e-02,\n",
      "          2.3200e-01, -2.0905e-01, -3.0394e-01, -4.0208e-03,  1.0978e-01,\n",
      "          6.5191e-02, -2.3204e-01,  6.3296e-02,  7.2149e-02, -1.3700e-01,\n",
      "         -1.4028e-01, -5.5573e-02, -1.0509e-01,  1.8968e-01, -8.1738e-02,\n",
      "         -1.7273e-01,  1.3207e-01, -1.2220e-01, -2.9100e-01, -6.8547e-04,\n",
      "          4.9522e-02, -9.4158e-02, -1.9661e-01,  3.4159e-01, -3.6180e-02,\n",
      "          5.7660e-02, -1.0313e-01,  2.5924e-01,  1.5648e-01, -2.4450e-01,\n",
      "         -2.1097e-01, -1.1903e-01, -1.0442e-01]], device='cuda:0'))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'hidden_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m\n\u001b[1;32m     16\u001b[0m     predicted_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(predicted_probs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predicted_label, predicted_probs, attention_weights\n\u001b[0;32m---> 20\u001b[0m predicted_label, predicted_probs, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Label:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted_label)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Probabilities:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predicted_probs)\n",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m, in \u001b[0;36mget_model_output\u001b[0;34m(model, tokenizer, sequence, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m predicted_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mattentions\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'hidden_states'"
     ]
    }
   ],
   "source": [
    "def get_model_output(model, tokenizer, sequence, device):\n",
    "    inputs = tokenizer(sequence, padding='max_length', max_length=128, return_tensors='pt').to(device)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    print(input_ids)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_attentions=True)\n",
    "        print(outputs)\n",
    "        print(outputs.hidden_states.shape)\n",
    "\n",
    "    predicted_probs = torch.softmax(outputs.logits, dim=-1)\n",
    "    attention_weights = outputs.attentions\n",
    "    print(np.array(attention_weights).shape)\n",
    "\n",
    "    predicted_label = torch.argmax(predicted_probs, dim=-1).item()\n",
    "\n",
    "    return predicted_label, predicted_probs, attention_weights\n",
    "\n",
    "predicted_label, predicted_probs, attention_weights = get_model_output(model, tokenizer, sequence, device)\n",
    "print(\"Predicted Label:\", predicted_label)\n",
    "print(\"Predicted Probabilities:\", predicted_probs)\n",
    "print(\"Attention Weights:\", attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f668c2-2d54-49e3-bf51-b585ef1163c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.output_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d30f8-47f6-436f-bf90-12ec9414c03d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209cdba-9e3a-4883-9af8-ad91087d1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d29bc9d-d5df-4b0c-977b-f743e3d18145",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertUnpadAttention(\n",
      "            (self): BertUnpadSelfAttention(\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (mlp): BertGatedLinearUnitMLP(\n",
      "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
      "            (act): GELU(approximate='none')\n",
      "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d577b9b3-3764-4418-bbde-274343b2ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idntity = testing_indx_0/1\n",
    "sequence\n",
    "input_ids\n",
    "attention_weights\n",
    "embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8369f737-78e2-4830-a6cb-9670ace60dca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dna",
   "language": "python",
   "name": "dna"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
